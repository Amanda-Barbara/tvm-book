{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 性能度量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.ir import IRModule\n",
    "from tvm import relay\n",
    "from tvm.ir.transform import Pass\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import te\n",
    "from tvm.relay import analysis, transform, Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_opt_pass(expr, opt_pass):\n",
    "    assert isinstance(opt_pass, Pass)\n",
    "    mod = IRModule.from_expr(expr)\n",
    "    mod = transform.InferType()(mod)\n",
    "    mod = opt_pass(mod)\n",
    "    entry = mod[\"main\"]\n",
    "    return entry if isinstance(expr, Function) else entry.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[08:58:06] /media/pc/data/4tb/lxw/books/tvm/src/relay/analysis/mac_count.cc:172: This pass only counts MACs in direct conv2d, conv2d_transpose, dense, and batch_matmul ops\n",
      "[08:58:06] /media/pc/data/4tb/lxw/books/tvm/src/relay/analysis/mac_count.cc:172: This pass only counts MACs in direct conv2d, conv2d_transpose, dense, and batch_matmul ops\n",
      "[08:58:06] /media/pc/data/4tb/lxw/books/tvm/src/relay/analysis/mac_count.cc:172: This pass only counts MACs in direct conv2d, conv2d_transpose, dense, and batch_matmul ops\n",
      "[08:58:06] /media/pc/data/4tb/lxw/books/tvm/src/relay/analysis/mac_count.cc:172: This pass only counts MACs in direct conv2d, conv2d_transpose, dense, and batch_matmul ops\n",
      "[08:58:06] /media/pc/data/4tb/lxw/books/tvm/src/relay/analysis/mac_count.cc:172: This pass only counts MACs in direct conv2d, conv2d_transpose, dense, and batch_matmul ops\n"
     ]
    }
   ],
   "source": [
    "def test_gemm():\n",
    "    n = 512\n",
    "    k = 1024\n",
    "    m = 256\n",
    "    dshape1 = (n, k)\n",
    "    dshape2 = (m, k)\n",
    "    data1 = relay.var(\"data1\", shape=dshape1)\n",
    "    data2 = relay.var(\"data2\", shape=dshape2)\n",
    "    gemm = relay.nn.dense(data1, data2)\n",
    "    func = relay.Function([data1, data2], relay.Tuple(tvm.runtime.convert([gemm])))\n",
    "    func = run_opt_pass(func, transform.InferType())\n",
    "    compute_count = analysis.get_total_mac_number(func)\n",
    "    expect_count = n * m * k\n",
    "    assert compute_count == expect_count\n",
    "\n",
    "def test_conv():\n",
    "    batch_size = 1\n",
    "    input_channel = 3\n",
    "    h = 224\n",
    "    w = 224\n",
    "    output_channel = 64\n",
    "    kh = 7\n",
    "    kw = 7\n",
    "    h_padding = 1\n",
    "    w_padding = 1\n",
    "    oh = h + h_padding * 2 - kh + 1\n",
    "    ow = w + w_padding * 2 - kw + 1\n",
    "    dshape = (batch_size, input_channel, h, w)\n",
    "    weight = relay.var(\"weight\", shape=(output_channel, input_channel, kh, kw))\n",
    "    data = relay.var(\"data\", shape=dshape)\n",
    "    conv2d = relay.nn.conv2d(\n",
    "        data, weight, channels=output_channel, kernel_size=(kh, kw), padding=(h_padding, w_padding)\n",
    "    )\n",
    "    func = relay.Function([data, weight], relay.Tuple(tvm.runtime.convert([conv2d])))\n",
    "    func = run_opt_pass(func, transform.InferType())\n",
    "    compute_count = analysis.get_total_mac_number(func)\n",
    "    expect_count = batch_size * input_channel * oh * ow * output_channel * kh * kw\n",
    "    assert compute_count == expect_count\n",
    "\n",
    "\n",
    "\n",
    "def test_simple_network():\n",
    "    batch_size = 1\n",
    "    dshape = (batch_size, 64, 56, 56)\n",
    "    weight_conv = relay.var(\"weight_conv\", shape=(64, 64, 3, 3))\n",
    "    data1 = relay.var(\"data1\", shape=dshape)\n",
    "    data2 = relay.var(\"data2\", shape=dshape)\n",
    "    weight_dense = relay.var(\"weight_dense\", shape=(1, 56 * 56 * 64))\n",
    "\n",
    "    conv2d_1 = relay.nn.conv2d(data1, weight_conv, channels=64, kernel_size=(3, 3), padding=(1, 1))\n",
    "    conv2d_2 = relay.nn.conv2d(data2, weight_conv, channels=64, kernel_size=(3, 3), padding=(1, 1))\n",
    "    add = relay.add(conv2d_1, conv2d_2)\n",
    "    flattened = relay.nn.batch_flatten(add)\n",
    "    dense_1 = relay.nn.dense(flattened, weight_dense)\n",
    "\n",
    "    func = relay.Function(\n",
    "        [data1, data2, weight_conv, weight_dense],\n",
    "        relay.Tuple(tvm.runtime.convert([conv2d_1, conv2d_2, dense_1, add, flattened])),\n",
    "    )\n",
    "    # alter the CONV 2D data layout to test\n",
    "    func = run_opt_pass(func, transform.AlterOpLayout())\n",
    "    compute_count = analysis.get_total_mac_number(func)\n",
    "    expect_count = 231411712\n",
    "    assert compute_count == expect_count\n",
    "\n",
    "\n",
    "def test_depthwise_conv2d():\n",
    "    batch_size = 1\n",
    "    dshape = (batch_size, 64, 56, 56)\n",
    "    weight_conv = relay.var(\"weight_depthwiseconv\", shape=(64, 1, 3, 3))\n",
    "    data1 = relay.var(\"data1\", shape=dshape)\n",
    "    data2 = relay.var(\"data2\", shape=dshape)\n",
    "    depthwise_conv2d_1 = relay.nn.conv2d(\n",
    "        data1, weight_conv, kernel_size=(3, 3), padding=(1, 1), groups=64\n",
    "    )\n",
    "    depthwise_conv2d_2 = relay.nn.conv2d(\n",
    "        data2, weight_conv, kernel_size=(3, 3), padding=(1, 1), groups=64\n",
    "    )\n",
    "    add = relay.add(depthwise_conv2d_1, depthwise_conv2d_2)\n",
    "    func = relay.Function(\n",
    "        [data1, data2, weight_conv],\n",
    "        relay.Tuple(tvm.runtime.convert([depthwise_conv2d_1, depthwise_conv2d_2, add])),\n",
    "    )\n",
    "    func = run_opt_pass(func, transform.InferType())\n",
    "    compute_count = analysis.get_total_mac_number(func)\n",
    "    assert compute_count == 2 * np.prod(dshape) * 3 * 3\n",
    "\n",
    "\n",
    "def test_conv_2d_transpose():\n",
    "    batch_size = 1\n",
    "    input_channel = 3\n",
    "    h = 224\n",
    "    w = 224\n",
    "    output_channel = 64\n",
    "    kh = 7\n",
    "    kw = 7\n",
    "    h_padding = 1\n",
    "    w_padding = 1\n",
    "    oh = h - h_padding * 2 + kh - 1\n",
    "    ow = w - w_padding * 2 + kw - 1\n",
    "    dshape = (batch_size, input_channel, h, w)\n",
    "    weight = relay.var(\"weight\", shape=(input_channel, output_channel, kh, kw))\n",
    "    data = relay.var(\"data\", shape=dshape)\n",
    "    conv2d_transpose = relay.nn.conv2d_transpose(\n",
    "        data, weight, channels=output_channel, kernel_size=(kh, kw), padding=(h_padding, w_padding)\n",
    "    )\n",
    "    func = relay.Function([data, weight], relay.Tuple(tvm.runtime.convert([conv2d_transpose])))\n",
    "    func = run_opt_pass(func, transform.InferType())\n",
    "    compute_count = analysis.get_total_mac_number(func)\n",
    "    expect_count = batch_size * input_channel * oh * ow * output_channel * kh * kw\n",
    "    assert compute_count == expect_count\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_conv()\n",
    "    test_gemm()\n",
    "    test_simple_network()\n",
    "    test_depthwise_conv2d()\n",
    "    test_conv_2d_transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "import os\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import te, runtime\n",
    "import json\n",
    "import base64\n",
    "from tvm._ffi.base import py_str\n",
    "from tvm.relay.op import add\n",
    "from tvm import relay\n",
    "from tvm import rpc\n",
    "from tvm.contrib import utils, graph_executor\n",
    "\n",
    "\n",
    "def test_save_load():\n",
    "    x = np.ones((10, 2)).astype(\"float32\")\n",
    "    y = np.ones((1, 2, 3)).astype(\"float32\")\n",
    "    params = {\"x\": x, \"y\": y}\n",
    "    param_bytes = runtime.save_param_dict(params)\n",
    "    assert isinstance(param_bytes, bytearray)\n",
    "    param2 = relay.load_param_dict(param_bytes)\n",
    "    assert len(param2) == 2\n",
    "    np.testing.assert_equal(param2[\"x\"].numpy(), x)\n",
    "    np.testing.assert_equal(param2[\"y\"].numpy(), y)\n",
    "\n",
    "\n",
    "def test_ndarray_reflection():\n",
    "    # Make two `NDArrayWrapper`s that point to the same underlying array.\n",
    "    np_array = np.random.uniform(size=(10, 2)).astype(\"float32\")\n",
    "    tvm_array = tvm.nd.array(np_array)\n",
    "    param_dict = {\"x\": tvm_array, \"y\": tvm_array}\n",
    "    assert param_dict[\"x\"].same_as(param_dict[\"y\"])\n",
    "    # Serialize then deserialize `param_dict`.\n",
    "    deser_param_dict = relay.load_param_dict(runtime.save_param_dict(param_dict))\n",
    "    # Make sure the data matches the original data and `x` and `y` contain the same data.\n",
    "    np.testing.assert_equal(deser_param_dict[\"x\"].numpy(), tvm_array.numpy())\n",
    "    # Make sure `x` and `y` contain the same data.\n",
    "    np.testing.assert_equal(deser_param_dict[\"x\"].numpy(), deser_param_dict[\"y\"].numpy())\n",
    "\n",
    "\n",
    "def test_bigendian_rpc_param():\n",
    "    \"\"\"Test big endian rpc when there is a PowerPC RPC server available\"\"\"\n",
    "    host = os.environ.get(\"TVM_POWERPC_TEST_HOST\", None)\n",
    "    port = os.environ.get(\"TVM_POWERPC_TEST_PORT\", 9090)\n",
    "    if host is None:\n",
    "        return\n",
    "\n",
    "    def verify_graph_executor(remote, target, shape, dtype):\n",
    "        x = relay.var(\"x\")\n",
    "        y = relay.const(1)\n",
    "        z = relay.add(x, y)\n",
    "        func = relay.Function([x], z)\n",
    "\n",
    "        x_in = np.ones(shape).astype(dtype)\n",
    "        params = {\"x\": x_in}\n",
    "        graph, lib, params = relay.build(func, target=target, params=params)\n",
    "\n",
    "        temp = utils.tempdir()\n",
    "        path_dso = temp.relpath(\"dev_lib.o\")\n",
    "        lib.save(path_dso)\n",
    "        remote.upload(path_dso)\n",
    "        lib = remote.load_module(\"dev_lib.o\")\n",
    "        dev = remote.cpu(0)\n",
    "        mod = graph_executor.create(graph, lib, dev)\n",
    "        mod.load_params(runtime.save_param_dict(params))\n",
    "        mod.run()\n",
    "        out = mod.get_output(0, tvm.nd.empty(shape, dtype=dtype, device=dev))\n",
    "        tvm.testing.assert_allclose(x_in + 1, out.numpy())\n",
    "\n",
    "    print(\"Test RPC connection to PowerPC...\")\n",
    "    remote = rpc.connect(host, port)\n",
    "    target = \"llvm -mtriple=powerpc-linux-gnu\"\n",
    "    for dtype in [\"float32\", \"float64\", \"int32\", \"int8\"]:\n",
    "        verify_graph_executor(remote, target, (10,), dtype)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_save_load()\n",
    "    test_ndarray_reflection()\n",
    "    test_bigendian_rpc_param()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20e538bd0bbffa4ce75068aaf85df10d4944f3fdb705eeec6781a4702773116f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
