{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动量化\n",
    "\n",
    "以 PyTorch 前端为例阐述 TVM 自动量化机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.runtime.vm import VirtualMachine\n",
    "from tvm import relay\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建单层卷积："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv = nn.Conv2d(16, 16, 3, 1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TVM 接受 {func}`torch.jit.trace` 后的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* span=aten___convolution_0_data:0:0 */, %aten___convolution_0_weight: Tensor[(16, 16, 3, 3), float32] /* span=aten___convolution_0_weight:0:0 */, %aten___convolution_0_bias: Tensor[(16), float32] /* span=aten___convolution_0_bias:0:0 */) {\n",
      "  %0 = nn.conv2d(%data, %aten___convolution_0_weight, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, %aten___convolution_0_bias) /* span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* span=aten__relu_0:0:0 */\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pt_model = Model().eval().float()\n",
    "ishape = (1, 16, 4, 4)\n",
    "input_shapes = [(\"data\", ishape)]\n",
    "# script_module = torch.jit.script(pt_model)\n",
    "# mod, params = relay.frontend.from_pytorch(script_module, input_shapes)\n",
    "idata = torch.rand(ishape)\n",
    "traced_model = torch.jit.trace(pt_model, idata)\n",
    "# traced_model 翻译为 TVM 前端模型\n",
    "mod, params = relay.frontend.from_pytorch(traced_model, input_shapes, use_parser_friendly_name=True)\n",
    "print(mod[\"main\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化 TVM 模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = multiply(%data, 16f /* ty=float32 */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %1 = round(%0) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %2 = clip(%1, a_min=-127f, a_max=127f) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %3 = cast(%2, dtype=\"int8\") /* ty=Tensor[(1, 16, 4, 4), int8] */;\n",
      "  %4 = nn.conv2d(%3, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), int8] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3], out_dtype=\"int32\") /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %5 = add(%4, 512 /* ty=int32 */) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %6 = right_shift(%5, 10 /* ty=int32 */) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %7 = clip(%6, a_min=-127f, a_max=127f) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %8 = cast(%7, dtype=\"int8\") /* ty=Tensor[(1, 16, 4, 4), int8] */;\n",
      "  %9 = annotation.stop_fusion(%8) /* ty=Tensor[(1, 16, 4, 4), int8] */;\n",
      "  %10 = cast(%9, dtype=\"float32\") /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %11 = multiply(%10, 0.0625f /* ty=float32 */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %12 = nn.bias_add(%11, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%12) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with relay.quantize.qconfig(skip_conv_layers=[]):\n",
    "    qmod = relay.quantize.quantize(mod, params)\n",
    "print(qmod[\"main\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = tvm.cpu()\n",
    "data_np = np.random.uniform(low=-1, high=1, size=[1, 16, 4, 4]).astype(\"float32\")\n",
    "input_dict = {\"data\": data_np}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化前结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    vm_exec = relay.vm.compile(mod, target=\"llvm\", params=params)\n",
    "vm = VirtualMachine(vm_exec, dev)\n",
    "vm.set_input(\"main\", **input_dict)\n",
    "tvm_res = vm.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化后结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    qvm_exec = relay.vm.compile(qmod, target=\"llvm\", params=params)\n",
    "qvm = VirtualMachine(qvm_exec, dev)\n",
    "qvm.set_input(\"main\", **input_dict)\n",
    "tvm_qres = qvm.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比 Torch 结果与 TVM 浮点结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch_res = traced_model(torch.from_numpy(data_np))\n",
    "np.testing.assert_allclose(\n",
    "    tvm_res.numpy(), torch_res.numpy(),\n",
    "    rtol=1e-5, atol=1e-5\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看量化前后的余弦相似度与 $L2$ 损失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm_book.testing.metric import cosine_similarity, l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9970607161521912, 0.00048657518345862627)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    cosine_similarity(tvm_res.numpy(), tvm_qres.numpy()), \n",
    "    l2_loss(tvm_res.numpy(), tvm_qres.numpy())\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 源码解析\n",
    "\n",
    "可以打印完整的量化流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行 pass: The meta data of the pass - pass name: sequential, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* span=aten___convolution_0_data:0:0 */) {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0], padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1]) /* span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> meta[IncompleteType][0] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* span=aten___convolution_0_data:0:0 */) {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0], padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1]) /* span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> meta[IncompleteType][0] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: SimplifyInference, opt_level: 0, required passes: [\n",
      "InferType, ]\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: FoldConstant, opt_level: 2, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: FoldScaleAxis, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: BackwardFoldScaleAxis, opt_level: 3, required passes: [\n",
      "InferType, ]\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: ForwardFoldScaleAxis, opt_level: 3, required passes: [\n",
      "InferType, ]\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: FoldConstant, opt_level: 2, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: CanonicalizeOps, opt_level: 3, required passes: [\n",
      "InferType, ]\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = expand_dims(meta[relay.Constant][1] /* ty=Tensor[(16), float32] */, axis=1, num_newaxis=2);\n",
      "  %2 = add(%0, %1) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%2) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: FoldConstant, opt_level: 2, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = expand_dims(meta[relay.Constant][1] /* ty=Tensor[(16), float32] */, axis=1, num_newaxis=2) /* ty=Tensor[(16, 1, 1), float32] */;\n",
      "  %2 = add(%0, %1) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%2) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 16, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = add(%0, meta[relay.Constant][1]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%1) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 16, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@tvm.instrument.pass_instrument\n",
    "class PrintIR:\n",
    "    def run_before_pass(self, mod, info):\n",
    "        print(f\"运行 pass: {info}\")\n",
    "        print(mod[\"main\"])\n",
    "\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3, instruments=[PrintIR()]):\n",
    "    with relay.quantize.qconfig(skip_conv_layers=[]):\n",
    "        qmod = relay.quantize.quantize(mod, params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tvm.relay.qnn.op.qnn.simulated_quantize`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接口见：`tvm/relay/qnn/op/_qnn.py`：\n",
    "\n",
    "```python\n",
    "@register_compute(\"qnn.simulated_quantize\")\n",
    "def simulated_quantize_compute(attrs, inputs, output_type):\n",
    "    assert len(inputs) == 4\n",
    "    return [\n",
    "        topi.nn.simulated_quantize(\n",
    "            inputs[0], inputs[1], inputs[2], inputs[3], axis=attrs.get_int(\"axis\")\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "register_injective_schedule(\"qnn.simulated_quantize\")\n",
    "register_pattern(\"qnn.simulated_quantize\", OpPattern.ELEMWISE)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出真正的实现见：{func}`tvm.topi.nn.simulated_quantize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulated_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_zero_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;34m@\u001b[0m\u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mELEMWISE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0msimulated_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_zero_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Simulated QNN quantize operator that mimics QNN outputs without changing datatype.\u001b[0m\n",
      "\u001b[0;34m    The benefit of this operator over true QNN quantize is that this operator allows dynamic\u001b[0m\n",
      "\u001b[0;34m    datatype selection and can operate on both per-channel and scalar scales and zero points while\u001b[0m\n",
      "\u001b[0;34m    QNN quantize requires both of these to be fixed at compile time.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    data: tvm.te.Tensor\u001b[0m\n",
      "\u001b[0;34m        An N-D input tensor to the operator.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    out_dtype: tvm.te.Tensor\u001b[0m\n",
      "\u001b[0;34m        A scalar variable that indicates which datatype to simulate quantization with. Use\u001b[0m\n",
      "\u001b[0;34m        SQNN_DTYPE_TO_CODE to convert a dtype string into the corresponding variable\u001b[0m\n",
      "\u001b[0;34m        value.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    output_scale: tvm.te.Tensor, optional\u001b[0m\n",
      "\u001b[0;34m        A scalar tensor representing the scale to use when quantizing to integer datatypes.\u001b[0m\n",
      "\u001b[0;34m        When it contains more than a single value, N must match the number of channels in data.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    output_zero_point: tvm.te.Tensor, optional\u001b[0m\n",
      "\u001b[0;34m        A 1-D tensor representing the zero point to use when quantizing to integer datatypes.\u001b[0m\n",
      "\u001b[0;34m        When it contains more than a single value, N must match the number of channels in data.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    axis: int, optional\u001b[0m\n",
      "\u001b[0;34m        The channel axis for quantization. Default value is -1 which corresponds to the last axis.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# When disabled, just pass through the input values.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_compute_pass_through\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Simulate quantization for arbitrary integer datatypes. The computation for all datatypes is:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Q_output = clip((round(input_tensor/output_scale) + output_zero_point),\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m#                 out_dtype::min,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m#                 out_dtype::max)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_compute_intn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0moutput_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput_zero_point\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mconst_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mconst_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Use indexmod to handle both scalar and per-channel QNN parameters.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mscale_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mzp_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_zero_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0moutput_scale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscale_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutput_zero_point\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mzp_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mconst_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mconst_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Use an if chain to dynamically return the proper quantization based on the input datatype.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# This allows the op to compile once but apply different quantization approaches\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# using a variable datatype input.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_dispatch_sim_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpass_through_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_compute_pass_through\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mint8_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_then_else\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mout_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSQNN_DTYPE_TO_CODE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"int8\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0m_compute_intn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mpass_through_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0muint8_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_then_else\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mout_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSQNN_DTYPE_TO_CODE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0m_compute_intn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mint8_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mint32_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_then_else\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mout_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSQNN_DTYPE_TO_CODE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"int32\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0m_compute_intn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0muint8_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mint32_value\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_dispatch_sim_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      /media/pc/data/lxw/ai/tvm/xinetzone/__pypackages__/3.10/lib/tvm/topi/nn/qnn.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "tvm.topi.nn.simulated_quantize??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{func}`tvm.te.tag_scope` 是 TVM 中的函数，用于在编译时为张量运算添加标签。这些标签可以帮助我们更好地理解和调试代码。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import te\n",
    "n = te.var('n')\n",
    "m = te.var('m')\n",
    "l = te.var('l')\n",
    "A = te.placeholder((n, l), name='A')\n",
    "B = te.placeholder((m, l), name='B')\n",
    "k = te.reduce_axis((0, l), name='k')\n",
    "\n",
    "with tvm.te.tag_scope(tag='matmul'):\n",
    "    C = te.compute((n, m), lambda i, j: te.sum(A[i, k] * B[j, k], axis=k))\n",
    "\n",
    "# 或者直接用作装饰器\n",
    "@tvm.te.tag_scope(tag=\"conv\")\n",
    "def compute_relu(data):\n",
    "    return te.compute(data.shape, lambda *i: tvm.tir.Select(data(*i) < 0, 0.0, data(*i)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模拟 QNN 量化算子，可以模仿 QNN 输出而不改变数据类型。与真正的 QNN 量化相比，该算子的优势在于它允许动态选择数据类型，并且可以在通道级别和标量尺度以及零点上操作，而 QNN 量化要求这些在编译时必须固定。\n",
    "\n",
    "模拟任意整数数据类型的量化。所有数据类型的计算如下：\n",
    "\n",
    "$$\n",
    "Q_{output} = \\operatorname{clip}((\\operatorname{round}(input_{tensor}/output_{scale}) + output_{zero\\_point}),\n",
    "                out\\_dtype_{\\min},\n",
    "                out\\_dtype_{\\max})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆过程为 {func}`tvm.relay.qnn.op.qnn.simulated_dequantize`，计算公式为：\n",
    "\n",
    "$$\n",
    "DQ_{output} = (input - zero\\_point) * scale\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tvm.relay.qnn.op.qnn.simulated_dequantize(data, input_scale, input_zero_point, axis=-1, in_dtype='int8')>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvm.relay.qnn.op.qnn.simulated_dequantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tvm.topi.nn.qnn.simulated_dequantize(data, in_dtype, input_scale=None, input_zero_point=None, axis=-1)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvm.topi.nn.simulated_dequantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_zero_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_zero_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrounding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'None'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcompute_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'None'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mrequantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_zero_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_zero_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrounding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"None\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcompute_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"None\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int8\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"Requantized operator.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The requantize operator converts one quantized tensor representation to\u001b[0m\n",
      "\u001b[0;34m    another quantized tensor representation. For the output tensor, we are\u001b[0m\n",
      "\u001b[0;34m    provided with output scale and zero point. The computation is as follows\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Q_output = zp_output +  (scale_input)/(scale_output) * (Q_input - zp_input)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    data : tvm.relay.Expr\u001b[0m\n",
      "\u001b[0;34m        The input data to the operator.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    input_scale: tvm.relay.Expr\u001b[0m\n",
      "\u001b[0;34m        The quantization scale for the input tensor.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    input_zero_point: tvm.relay.Expr\u001b[0m\n",
      "\u001b[0;34m        The zero point of the input tensor.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    output_scale: tvm.relay.Expr\u001b[0m\n",
      "\u001b[0;34m        The quantization scale for the output tensor.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    output_zero_point: tvm.relay.Expr\u001b[0m\n",
      "\u001b[0;34m        The zero point of the output tensor.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    axis : int\u001b[0m\n",
      "\u001b[0;34m        The channel axis for quantization. Default value is -1 which corresponds to the last axis.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    rounding : string, optional\u001b[0m\n",
      "\u001b[0;34m        Defines the rounding direction when the value is midway between two\u001b[0m\n",
      "\u001b[0;34m        representable values.\u001b[0m\n",
      "\u001b[0;34m    compute_dtype:\u001b[0m\n",
      "\u001b[0;34m        Specifies the data type used during requantize.\u001b[0m\n",
      "\u001b[0;34m        Supported options: \\\"int64\\\", \\\"float32\\\", \\\"float64\\\"\u001b[0m\n",
      "\u001b[0;34m    out_dtype : str, optional\u001b[0m\n",
      "\u001b[0;34m        Specifies the output data type.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns\u001b[0m\n",
      "\u001b[0;34m    -------\u001b[0m\n",
      "\u001b[0;34m    result : tvm.relay.Expr\u001b[0m\n",
      "\u001b[0;34m        The computed result.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_make\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minput_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minput_zero_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput_zero_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrounding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcompute_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mout_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      /media/pc/data/lxw/ai/tvm/xinetzone/__pypackages__/3.10/lib/tvm/relay/qnn/op/qnn.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "tvm.relay.qnn.op.qnn.requantize??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重量化算子将量化张量表示转换为另一个量化张量表示。对于输出张量，提供了输出 scale 和零点。计算如下：\n",
    "\n",
    "$$\n",
    "Q_{output} = zp_{output} +  (scale_{input})/(scale_{output}) * (Q_{input} - zp_{input})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
