{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tvm\n",
    "from tvm import te\n",
    "from tvm import relay\n",
    "from tvm.relay import testing\n",
    "from tvm.relay.expr import Call\n",
    "from tvm.topi.utils import get_const_tuple\n",
    "\n",
    "\n",
    "def quantize_and_build(out, skip_conv_layers=[]):\n",
    "    f = relay.Function(relay.analysis.free_vars(out), out)\n",
    "    mod, params = testing.create_workload(f)\n",
    "\n",
    "    with relay.quantize.qconfig(skip_conv_layers=skip_conv_layers):\n",
    "        qmod = relay.quantize.quantize(mod, params)\n",
    "\n",
    "    relay.build(qmod, \"llvm\", params=params)\n",
    "    return mod, qmod"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 乘法算子的右操作数不是常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = relay.var(\"data\", shape=(1, 16, 64, 64))\n",
    "multiplier = relay.sigmoid(relay.var(\"data\", shape=(1, 16, 1, 1)))\n",
    "conv = relay.nn.conv2d(\n",
    "    data, relay.var(\"weight\"), kernel_size=(3, 3), padding=(1, 1), channels=16\n",
    ")\n",
    "act = relay.nn.relu(data=conv)\n",
    "mod, qmod = quantize_and_build(act * multiplier)\n",
    "pool = relay.nn.global_avg_pool2d(data=act)\n",
    "mod, qmod = quantize_and_build(act * pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 跳过卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = relay.var(\"data\", shape=(1, 16, 64, 64))\n",
    "np_weight = np.random.rand(16, 16, 3, 3)\n",
    "conv0_weight = relay.Constant(tvm.nd.array(np_weight)).astype(\"float32\")\n",
    "conv1_weight = relay.Constant(tvm.nd.array(np_weight)).astype(\"float32\")\n",
    "multiplier = relay.sigmoid(relay.var(\"data\", shape=(1, 16, 1, 1)))\n",
    "\n",
    "conv0 = relay.nn.conv2d(data, conv0_weight, kernel_size=(3, 3), padding=(1, 1), channels=16)\n",
    "act0 = relay.nn.relu(data=conv0)\n",
    "conv1 = relay.nn.conv2d(act0, conv1_weight, kernel_size=(3, 3), padding=(1, 1), channels=16)\n",
    "act1 = relay.nn.relu(data=conv1)\n",
    "\n",
    "quantize_and_build(act1 * multiplier)\n",
    "quantize_and_build(act1 * multiplier, skip_conv_layers=[0])\n",
    "quantize_and_build(act1 * multiplier, skip_conv_layers=[1])\n",
    "mod, qmod = quantize_and_build(act1 * multiplier, skip_conv_layers=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `stop_quantize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = relay.var(\"data\", shape=(1, 16, 64, 64))\n",
    "np_weight0 = np.random.rand(16, 16, 3, 3)\n",
    "conv0_weight = relay.Constant(tvm.nd.array(np_weight0)).astype(\"float32\")\n",
    "np_weight1 = np.random.rand(16, 16, 1, 1)\n",
    "conv1_weight = relay.Constant(tvm.nd.array(np_weight1)).astype(\"float32\")\n",
    "multiplier = relay.sigmoid(relay.var(\"data\", shape=(1, 16, 1, 1)))\n",
    "\n",
    "conv0 = relay.nn.conv2d(data, conv0_weight, kernel_size=(3, 3), padding=(1, 1), channels=16)\n",
    "act0 = relay.nn.relu(data=conv0)\n",
    "\n",
    "pool = relay.nn.global_avg_pool2d(data=act0)\n",
    "\n",
    "conv1 = relay.nn.conv2d(pool, conv1_weight, kernel_size=(1, 1), padding=(0, 0), channels=16)\n",
    "act1 = relay.nn.relu(data=conv1)\n",
    "\n",
    "mod, qmod = quantize_and_build(act1 * multiplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `batch_flatten`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = relay.var(\"data\", shape=(1, 16, 64, 64), dtype=\"float32\")\n",
    "\n",
    "out = relay.nn.conv2d(\n",
    "    data, relay.var(\"weight\"), kernel_size=(3, 3), padding=(1, 1), channels=16\n",
    ")\n",
    "\n",
    "out = relay.nn.batch_flatten(out)\n",
    "\n",
    "mod, qmod = quantize_and_build(out)\n",
    "\n",
    "def _check_batch_flatten(node):\n",
    "    if isinstance(node, Call):\n",
    "        if node.op.name == \"nn.batch_flatten\":\n",
    "            assert node.checked_type.dtype == \"int8\"\n",
    "\n",
    "# check if batch_flatten is quantized\n",
    "relay.analysis.post_order_visit(qmod[\"main\"], _check_batch_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `batch_matmul`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = relay.var(\"data\", shape=(1, 4, 16, 16))\n",
    "data2 = relay.sigmoid(relay.var(\"data\", shape=(4, 16, 64)))\n",
    "out = relay.nn.conv2d(data, relay.var(\"weight\"), kernel_size=(3, 3), padding=(1, 1), channels=8)\n",
    "\n",
    "out = relay.nn.batch_flatten(out)\n",
    "out = relay.reshape(out, [1, 32, 64])\n",
    "out = relay.nn.batch_matmul(out, data2)\n",
    "\n",
    "mod, qmod = quantize_and_build(out)\n",
    "\n",
    "def _check_batch_matmul(node):\n",
    "    if isinstance(node, Call):\n",
    "\n",
    "        if node.op.name in [\"nn.batch_matmul\", \"nn.conv2d\"]:\n",
    "            assert node.checked_type.dtype == \"int32\"\n",
    "        elif node.op.name == \"nn.batch_flatten\":\n",
    "            assert node.checked_type.dtype == \"int8\"\n",
    "\n",
    "# check if batch_matmul is quantized\n",
    "relay.analysis.post_order_visit(qmod[\"main\"], _check_batch_matmul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `calibration_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibration_dataset(mod, input_name):\n",
    "    dataset = []\n",
    "    input_shape = [int(x) for x in mod[\"main\"].checked_type.arg_types[0].shape]\n",
    "    for i in range(5):\n",
    "        data = np.random.uniform(size=input_shape)\n",
    "        dataset.append({input_name: data})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, params = testing.synthetic.get_workload()\n",
    "dataset = get_calibration_dataset(mod, \"data\")\n",
    "create_target = True\n",
    "with relay.quantize.qconfig(calibrate_mode=\"kl_divergence\"):\n",
    "    if create_target:\n",
    "        with tvm.target.Target(\"llvm\"):\n",
    "            relay.quantize.quantize(mod, params, dataset)\n",
    "    else:\n",
    "        # current_target = None\n",
    "        relay.quantize.quantize(mod, params, dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calibrate_memory_bound`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, params = testing.synthetic.get_workload()\n",
    "dataset = get_calibration_dataset(mod, \"data\")\n",
    "import multiprocessing\n",
    "\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "with relay.quantize.qconfig(calibrate_mode=\"kl_divergence\", calibrate_chunk_by=num_cpu):\n",
    "    relay.quantize.quantize(mod, params, dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calibrate_percentile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, params = testing.synthetic.get_workload()\n",
    "dataset = get_calibration_dataset(mod, \"data\")\n",
    "with relay.quantize.qconfig(calibrate_mode=\"percentile\"):\n",
    "    relay.quantize.quantize(mod, params, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化与反量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CFG = {\n",
    "    \"skip_conv_layers\": [],\n",
    "    \"skip_dense_layers\": False,\n",
    "    \"dtype_input\": \"int8\",\n",
    "    \"dtype_weight\": \"int8\",\n",
    "    \"dtype_activation\": \"int32\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_tvm(tt, low, high):\n",
    "    if \"int\" in tt.dtype:\n",
    "        data_np = np.random.randint(low, high, size=get_const_tuple(tt.shape), dtype=tt.dtype)\n",
    "    elif \"float\" in tt.dtype:\n",
    "        data_np = np.random.uniform(low, high, size=get_const_tuple(tt.shape)).astype(tt.dtype)\n",
    "    else:\n",
    "        assert False, \"unknown dtype\"\n",
    "    return tvm.nd.array(data_np, device=tvm.cpu(0))\n",
    "\n",
    "def verify_partition_fails(mod, params):\n",
    "    # standard partition should always succeed\n",
    "    with relay.quantize.qconfig(**BASE_CFG, partition_conversions=\"enabled\"):\n",
    "        partitioned_mod = relay.quantize.quantize(mod, params)\n",
    "\n",
    "    try:\n",
    "        with relay.quantize.qconfig(**BASE_CFG, partition_conversions=\"fully_integral\"):\n",
    "            partitioned_mod = relay.quantize.quantize(mod, params)\n",
    "        raise RuntimeError(\"partitioning should have failed\")\n",
    "    except AssertionError:\n",
    "        pass\n",
    "\n",
    "def verify_partition(mod, params):\n",
    "    with relay.quantize.qconfig(**BASE_CFG, paritition_conversions=\"disabled\"):\n",
    "        unpartitioned_mod = relay.quantize.quantize(mod, params)\n",
    "        assert (\n",
    "            len(unpartitioned_mod.get_global_vars()) == 1\n",
    "        ), \"unpartitioned module should only have one function\"\n",
    "    with relay.quantize.qconfig(**BASE_CFG, partition_conversions=\"fully_integral\"):\n",
    "        partitioned_mod = relay.quantize.quantize(mod, params)\n",
    "\n",
    "    # ensure partitioned and unpartitioned results agree\n",
    "    params = [gen_rand_tvm(param.type_annotation, 0, 1) for param in partitioned_mod[\"main\"].params]\n",
    "\n",
    "    def _eval_mod(mod):\n",
    "        return relay.create_executor(\"vm\", device=tvm.cpu(0), target=\"llvm\", mod=mod).evaluate()(\n",
    "            *params\n",
    "        )\n",
    "\n",
    "    partitioned_mod_result = _eval_mod(partitioned_mod)\n",
    "    unpartitioned_mod_result = _eval_mod(unpartitioned_mod)\n",
    "    tvm.testing.assert_allclose(unpartitioned_mod_result.numpy(), partitioned_mod_result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_partition():\n",
    "    mod = tvm.relay.parse(\n",
    "        \"\"\"\n",
    "    #[version = \"0.0.5\"]\n",
    "    def @main(\n",
    "        %x: Tensor[(10, 10), float32],\n",
    "        %y: Tensor[(10, 10), float32]) {\n",
    "      add(%x, %y)\n",
    "    }\n",
    "    \"\"\"\n",
    "    )\n",
    "    params = {}\n",
    "    verify_partition_fails(mod, params)\n",
    "\n",
    "\n",
    "def test_conv2d_partition():\n",
    "    mod = tvm.relay.parse(\n",
    "        \"\"\"\n",
    "    #[version = \"0.0.5\"]\n",
    "    def @main(\n",
    "        %x: Tensor[(1, 4, 16, 16), float32],\n",
    "        %w: Tensor[(4, 4, 3, 3), float32]) -> Tensor[(1, 4, 16, 16), float32] {\n",
    "      nn.conv2d(%x, %w,\n",
    "        padding=[1, 1, 1, 1],\n",
    "        channels=4,\n",
    "        kernel_size=[3, 3])\n",
    "    }\n",
    "    \"\"\"\n",
    "    )\n",
    "    weight_ty = mod[\"main\"].params[1].checked_type\n",
    "    params = {\"w\": gen_rand_tvm(weight_ty, 0, 1)}\n",
    "    verify_partition(mod, params)\n",
    "\n",
    "\n",
    "def test_multiple_arg_conversions_partition():\n",
    "    mod = tvm.relay.parse(\n",
    "        \"\"\"\n",
    "    #[version = \"0.0.5\"]\n",
    "    def @main(\n",
    "        %x1: Tensor[(1, 4, 16, 16), float32],\n",
    "        %w1: Tensor[(4, 4, 3, 3), float32],\n",
    "        %x2: Tensor[(1, 4, 16, 16), float32],\n",
    "        %w2: Tensor[(4, 4, 3, 3), float32]\n",
    "        ) -> Tensor[(1, 4, 16, 16), float32] {\n",
    "      %0 = nn.conv2d(%x1, %w1,\n",
    "        padding=[1, 1, 1, 1],\n",
    "        channels=4,\n",
    "        kernel_size=[3, 3]);\n",
    "      %1 = nn.conv2d(%x2, %w2,\n",
    "        padding=[1, 1, 1, 1],\n",
    "        channels=4,\n",
    "        kernel_size=[3, 3]);\n",
    "      add(%0, %1)\n",
    "    }\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    w1_ty = mod[\"main\"].params[1].checked_type\n",
    "    w2_ty = mod[\"main\"].params[3].checked_type\n",
    "    params = {\"w1\": gen_rand_tvm(w1_ty, 0, 1), \"w2\": gen_rand_tvm(w2_ty, 0, 1)}\n",
    "    verify_partition(mod, params)\n",
    "\n",
    "\n",
    "def test_unquantizable_prefix_partition():\n",
    "    mod = tvm.relay.parse(\n",
    "        \"\"\"\n",
    "    #[version = \"0.0.5\"]\n",
    "    def @main(\n",
    "        %x: Tensor[(1, 4, 16, 16), float32],\n",
    "        %b: Tensor[(4), float32],\n",
    "        %w: Tensor[(4, 4, 3, 3), float32]) -> Tensor[(1, 4, 16, 16), float32] {\n",
    "      // NOTE bias_add isn't currently quantizable\n",
    "      %0 = nn.bias_add(%x, %b);\n",
    "      nn.conv2d(%0, %w,\n",
    "        padding=[1, 1, 1, 1],\n",
    "        channels=4,\n",
    "        kernel_size=[3, 3])\n",
    "    }\n",
    "    \"\"\"\n",
    "    )\n",
    "    bias_ty = mod[\"main\"].params[1].checked_type\n",
    "    weight_ty = mod[\"main\"].params[2].checked_type\n",
    "    params = {\"b\": gen_rand_tvm(bias_ty, 0, 1), \"w\": gen_rand_tvm(weight_ty, 0, 1)}\n",
    "    verify_partition_fails(mod, params)\n",
    "\n",
    "\n",
    "def test_unquantizable_core_partition():\n",
    "    mod = tvm.relay.parse(\n",
    "        \"\"\"\n",
    "    #[version = \"0.0.5\"]\n",
    "    def @main(\n",
    "        %x1: Tensor[(1, 4, 16, 16), float32],\n",
    "        %w1: Tensor[(4, 4, 3, 3), float32],\n",
    "        %b: Tensor[(4), float32],\n",
    "        %w2: Tensor[(4, 4, 3, 3), float32]) -> Tensor[(1, 4, 16, 16), float32] {\n",
    "      %0 = nn.conv2d(%x1, %w1,\n",
    "        padding=[1, 1, 1, 1],\n",
    "        channels=4,\n",
    "        kernel_size=[3, 3]);\n",
    "      // NOTE bias_add isn't currently quantizable\n",
    "      %1 = nn.bias_add(%0, %b);\n",
    "      nn.conv2d(%1, %w2,\n",
    "        padding=[1, 1, 1, 1],\n",
    "        channels=4,\n",
    "        kernel_size=[3, 3])\n",
    "    }\n",
    "    \"\"\"\n",
    "    )\n",
    "    w1_ty = mod[\"main\"].params[1].checked_type\n",
    "    bias_ty = mod[\"main\"].params[2].checked_type\n",
    "    w2_ty = mod[\"main\"].params[3].checked_type\n",
    "    params = {\n",
    "        \"w1\": gen_rand_tvm(w1_ty, 0, 1),\n",
    "        \"w2\": gen_rand_tvm(w2_ty, 0, 1),\n",
    "        \"b\": gen_rand_tvm(bias_ty, 0, 1),\n",
    "    }\n",
    "    verify_partition_fails(mod, params)\n",
    "\n",
    "\n",
    "def test_unquantizable_suffix_partition():\n",
    "    mod = tvm.relay.parse(\n",
    "        \"\"\"\n",
    "    #[version = \"0.0.5\"]\n",
    "    def @main(\n",
    "        %x: Tensor[(1, 4, 16, 16), float32],\n",
    "        %w: Tensor[(4, 4, 3, 3), float32],\n",
    "        %b: Tensor[(4), float32]) -> Tensor[(1, 4, 16, 16), float32] {\n",
    "      %0 = nn.conv2d(%x, %w,\n",
    "        padding=[1, 1, 1, 1],\n",
    "        channels=4,\n",
    "        kernel_size=[3, 3]);\n",
    "      // NOTE bias_add isn't currently quantizable\n",
    "      nn.bias_add(%0, %b)\n",
    "    }\n",
    "    \"\"\"\n",
    "    )\n",
    "    weight_ty = mod[\"main\"].params[1].checked_type\n",
    "    bias_ty = mod[\"main\"].params[2].checked_type\n",
    "    params = {\"w\": gen_rand_tvm(weight_ty, 0, 1), \"b\": gen_rand_tvm(bias_ty, 0, 1)}\n",
    "    verify_partition_fails(mod, params)\n",
    "\n",
    "\n",
    "def test_left_shift_negative():\n",
    "    data = relay.var(\"data\", shape=(1, 16, 64, 64))\n",
    "    weight = relay.const(np.full((16, 16, 3, 3), 256.0))\n",
    "    conv2d = relay.nn.conv2d(data, weight, kernel_size=(3, 3), padding=(1, 1), channels=16)\n",
    "    relu = relay.nn.relu(conv2d)\n",
    "\n",
    "    mod = tvm.IRModule.from_expr(relu)\n",
    "\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        with relay.quantize.qconfig(\n",
    "            calibrate_mode=\"global_scale\", global_scale=8.0, skip_conv_layers=None\n",
    "        ):\n",
    "            qnn_mod = relay.quantize.quantize(mod)\n",
    "\n",
    "    class OpFinder(relay.ExprVisitor):\n",
    "        def __init__(self, op_name):\n",
    "            super(OpFinder, self).__init__()\n",
    "            self._op_name = op_name\n",
    "            self.ops = list()\n",
    "\n",
    "        def visit_call(self, call):\n",
    "            super().visit_call(call)\n",
    "            if call.op.name == self._op_name:\n",
    "                self.ops.append(call)\n",
    "\n",
    "    opf = OpFinder(\"left_shift\")\n",
    "    opf.visit(qnn_mod[\"main\"])\n",
    "    assert len(opf.ops) > 0, 'Broken case, can\\'t find any \"left_shift\" operators.'\n",
    "    for left_shift_op in opf.ops:\n",
    "        shift_amount = left_shift_op.args[1].data.numpy()\n",
    "        assert shift_amount >= 0, \"Shift amount must be non-negative.\"\n",
    "\n",
    "\n",
    "def test_dense_conv2d_rewrite():\n",
    "    n, c, h, w = 1, 16, 64, 64\n",
    "    data = relay.var(\"data\", relay.TensorType((n, c, h, w)))\n",
    "    inp = relay.var(\"inp\", relay.TensorType((n, c * h * w)))\n",
    "    weight_T = relay.const(np.random.random((n, c * h * w)), dtype=\"float32\")\n",
    "    bias = relay.const(np.random.random((n,)), dtype=\"float32\")\n",
    "    conv_w = relay.const(np.random.random((16, 16, 3, 3)), dtype=\"float32\")\n",
    "\n",
    "    dense_o = relay.nn.dense(inp, weight_T)\n",
    "    linear_o = relay.nn.bias_add(dense_o, bias)\n",
    "    conv2d_o = relay.nn.conv2d(data, conv_w, kernel_size=(3, 3), padding=(1, 1), channels=16)\n",
    "    result = relay.Tuple((linear_o, conv2d_o))\n",
    "\n",
    "    mod = tvm.IRModule.from_expr(result)\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        with relay.quantize.qconfig(\n",
    "            calibrate_mode=\"global_scale\", global_scale=8.0, skip_dense_layer=False\n",
    "        ):\n",
    "            qnn_mod = relay.quantize.quantize(mod)\n",
    "\n",
    "    def _check_dense(node):\n",
    "        if isinstance(node, Call):\n",
    "            if node.op.name == \"nn.dense\":\n",
    "                assert node.args[0].checked_type.dtype == \"int8\"\n",
    "                assert node.args[1].checked_type.dtype == \"int8\"\n",
    "                assert node.checked_type.dtype == \"int32\"\n",
    "            if node.op.name == \"nn.conv2d\":\n",
    "                assert node.args[0].checked_type.dtype == \"float32\"\n",
    "                assert node.args[1].checked_type.dtype == \"float32\"\n",
    "                assert node.checked_type.dtype == \"float32\"\n",
    "\n",
    "    relay.analysis.post_order_visit(qnn_mod[\"main\"], _check_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
