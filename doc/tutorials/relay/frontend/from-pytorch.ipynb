{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 前端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.ao.quantization import get_default_qat_qconfig_mapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class Demo(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(16, 64, 3, 1, 1, bias=False, groups=16)\n",
    "        # self.prelu = nn.PReLU(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv(x)\n",
    "        # x = self.prelu(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torch/ao/quantization/utils.py:310: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Demo()\n",
    "shape = 1, 16, 32, 32\n",
    "example_inputs = [torch.rand(*shape),]\n",
    "# script_module = torch.jit.trace(model.eval(), example_inputs)\n",
    "model_qat = torch.fx.symbolic_trace(model)\n",
    "model_qat = torch.fx.GraphModule(model_qat, model_qat.graph)\n",
    "qconfig_mapping = get_default_qat_qconfig_mapping(\"qnnpack\")\n",
    "model_prepared = prepare_qat_fx(model_qat, qconfig_mapping, example_inputs).eval()\n",
    "model_converted = convert_fx(model_prepared).eval()\n",
    "script_module = torch.jit.trace(model_converted.eval(), example_inputs).eval()\n",
    "input_infos = [(\"data\", shape),]\n",
    "default_dtype = \"float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.relay.frontend.pytorch import from_pytorch\n",
    "\n",
    "input_infos = [(\"data\", shape),]\n",
    "mod, params = from_pytorch(\n",
    "    script_module, input_infos,\n",
    "    custom_convert_map=None,\n",
    "    default_dtype='float32',\n",
    "    use_parser_friendly_name=False,\n",
    "    keep_quantized_weight=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%data: Tensor[(1, 16, 32, 32), float32] /* span=aten::quantize_per_tensor_0.data:0:0 */, %conv_weight: Tensor[(64, 1, 3, 3), float32] /* span=quantized::conv2d_relu_0:0:0 */) {\n",
      "  %0 = qnn.quantize(%data, 1f /* span=aten::quantize_per_tensor_0:0:0 */, 0 /* span=aten::quantize_per_tensor_0:0:0 */, out_dtype=\"uint8\", axis=1) /* span=aten::quantize_per_tensor_0:0:0 */;\n",
      "  %1 = nn.pad(%0, 0f /* span=quantized::conv2d_relu_0:0:0 */, pad_width=[[0, 0], [0, 0], [1, 1], [1, 1]]) /* span=quantized::conv2d_relu_0:0:0 */;\n",
      "  %2 = qnn.quantize(%conv_weight, 0.00261353f /* span=quantized::conv2d_relu_0:0:0 */, 0 /* span=quantized::conv2d_relu_0:0:0 */, out_dtype=\"int8\", axis=0) /* span=quantized::conv2d_relu_0:0:0 */;\n",
      "  %3 = qnn.conv2d(%1, %2, 0 /* span=quantized::conv2d_relu_0:0:0 */, 0 /* span=quantized::conv2d_relu_0:0:0 */, 1f /* span=quantized::conv2d_relu_0:0:0 */, 0.00261353f /* span=quantized::conv2d_relu_0:0:0 */, padding=[0, 0, 0, 0], groups=16, channels=64, kernel_size=[3, 3], out_dtype=\"int32\") /* span=quantized::conv2d_relu_0:0:0 */;\n",
      "  %4 = qnn.requantize(%3, 0.00261353f /* span=quantized::conv2d_relu_0:0:0 */, 0 /* span=quantized::conv2d_relu_0:0:0 */, 1f /* span=quantized::conv2d_relu_0:0:0 */, 0 /* span=quantized::conv2d_relu_0:0:0 */, axis=1, out_dtype=\"int32\") /* span=quantized::conv2d_relu_0:0:0 */;\n",
      "  %5 = clip(%4, a_min=0f, a_max=255f) /* span=quantized::conv2d_relu_0:0:0 */;\n",
      "  %6 = cast(%5, dtype=\"uint8\") /* span=quantized::conv2d_relu_0:0:0 */;\n",
      "  qnn.dequantize(%6, 1f /* span=aten::dequantize_0:0:0 */, 0 /* span=aten::dequantize_0:0:0 */) /* span=aten::dequantize_0:0:0 */\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(mod[\"main\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = script_module.graph.copy()\n",
    "graph_inputs = list(graph.inputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[self.1 defined in (%self.1 : __torch__.torch.fx.graph_module.GraphModule, %x : Float(1, 16, 32, 32, strides=[16384, 1024, 32, 1], requires_grad=0, device=cpu) = prim::Param()\n",
       " ),\n",
       " x defined in (%self.1 : __torch__.torch.fx.graph_module.GraphModule, %x : Float(1, 16, 32, 32, strides=[16384, 1024, 32, 1], requires_grad=0, device=cpu) = prim::Param()\n",
       " )]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:04, 43.71it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Shapes of input list and information in the graph do not match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m script_model \u001b[39m=\u001b[39m convert_fx(model)\n\u001b[1;32m     30\u001b[0m input_infos \u001b[39m=\u001b[39m [(\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m, shape),]\n\u001b[0;32m---> 31\u001b[0m mod, params \u001b[39m=\u001b[39m from_pytorch(\n\u001b[1;32m     32\u001b[0m     script_module, input_infos,\n\u001b[1;32m     33\u001b[0m     custom_convert_map\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     34\u001b[0m     default_dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m     use_parser_friendly_name\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     36\u001b[0m     keep_quantized_weight\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m )\n",
      "File \u001b[0;32m/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/__pypackages__/3.10/lib/tvm/relay/frontend/pytorch.py:4939\u001b[0m, in \u001b[0;36mfrom_pytorch\u001b[0;34m(script_module, input_infos, custom_convert_map, default_dtype, use_parser_friendly_name, keep_quantized_weight, export_renamed_c_graph_path)\u001b[0m\n\u001b[1;32m   4937\u001b[0m is_module \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(script_module, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule)\n\u001b[1;32m   4938\u001b[0m params \u001b[39m=\u001b[39m script_module\u001b[39m.\u001b[39mstate_dict() \u001b[39mif\u001b[39;00m is_module \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 4939\u001b[0m outputs \u001b[39m=\u001b[39m _get_relay_input_vars(\n\u001b[1;32m   4940\u001b[0m     graph, input_infos, prelude, default_dtype\u001b[39m=\u001b[39;49mdefault_dtype, is_module\u001b[39m=\u001b[39;49mis_module\n\u001b[1;32m   4941\u001b[0m )\n\u001b[1;32m   4943\u001b[0m \u001b[39mif\u001b[39;00m use_parser_friendly_name:\n\u001b[1;32m   4944\u001b[0m     new_names \u001b[39m=\u001b[39m [key\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mkeys()]\n",
      "File \u001b[0;32m/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/__pypackages__/3.10/lib/tvm/relay/frontend/pytorch.py:4698\u001b[0m, in \u001b[0;36m_get_relay_input_vars\u001b[0;34m(graph, input_infos, prelude, is_module, default_dtype)\u001b[0m\n\u001b[1;32m   4695\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4696\u001b[0m         new_input_infos\u001b[39m.\u001b[39mappend(inp)\n\u001b[0;32m-> 4698\u001b[0m input_types \u001b[39m=\u001b[39m [\n\u001b[1;32m   4699\u001b[0m     (name, get_relay_ty(info[\u001b[39m0\u001b[39m], info[\u001b[39m1\u001b[39m], gi\u001b[39m.\u001b[39mtype()))\n\u001b[1;32m   4700\u001b[0m     \u001b[39mfor\u001b[39;00m (name, info), gi \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(new_input_infos, graph_inputs)\n\u001b[1;32m   4701\u001b[0m ]\n\u001b[1;32m   4703\u001b[0m ir_inputs \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39mdebugName() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m graph_inputs]\n\u001b[1;32m   4704\u001b[0m \u001b[39mfor\u001b[39;00m ir_input, (name, itype) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(ir_inputs, input_types):\n",
      "File \u001b[0;32m/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/__pypackages__/3.10/lib/tvm/relay/frontend/pytorch.py:4699\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4695\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4696\u001b[0m         new_input_infos\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m   4698\u001b[0m input_types \u001b[39m=\u001b[39m [\n\u001b[0;32m-> 4699\u001b[0m     (name, get_relay_ty(info[\u001b[39m0\u001b[39;49m], info[\u001b[39m1\u001b[39;49m], gi\u001b[39m.\u001b[39;49mtype()))\n\u001b[1;32m   4700\u001b[0m     \u001b[39mfor\u001b[39;00m (name, info), gi \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(new_input_infos, graph_inputs)\n\u001b[1;32m   4701\u001b[0m ]\n\u001b[1;32m   4703\u001b[0m ir_inputs \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39mdebugName() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m graph_inputs]\n\u001b[1;32m   4704\u001b[0m \u001b[39mfor\u001b[39;00m ir_input, (name, itype) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(ir_inputs, input_types):\n",
      "File \u001b[0;32m/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/__pypackages__/3.10/lib/tvm/relay/frontend/pytorch.py:4644\u001b[0m, in \u001b[0;36m_get_relay_input_vars.<locals>.get_relay_ty\u001b[0;34m(ishape, itype, pt_type)\u001b[0m\n\u001b[1;32m   4639\u001b[0m \u001b[39mif\u001b[39;00m (pt_type\u001b[39m.\u001b[39mdim() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m pt_type\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(ishape)) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   4640\u001b[0m     pt_type\u001b[39m.\u001b[39msizes() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4641\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m([s1 \u001b[39m!=\u001b[39m s2 \u001b[39mfor\u001b[39;00m s1, s2 \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(pt_type\u001b[39m.\u001b[39msizes(), ishape)])\n\u001b[1;32m   4642\u001b[0m ):\n\u001b[1;32m   4643\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mShapes of input list and information in the graph do not match\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 4644\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m   4645\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(ishape) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(dim \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m ishape[\u001b[39m1\u001b[39m:]):\n\u001b[1;32m   4646\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m   4647\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected input\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms non-batch dimensions to have positive length, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4648\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut input has a shape of \u001b[39m\u001b[39m{\u001b[39;00mpt_type\u001b[39m.\u001b[39msizes()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4649\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Shapes of input list and information in the graph do not match"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import get_default_qconfig_mapping #QConfigMapping\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from draft.resnet_sigmoid import resnet18\n",
    "from draft.dataset import Cifar10\n",
    "\n",
    "def calibrate(model, data_loader, num=200):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k, (image, _) in tqdm(enumerate(data_loader)):\n",
    "            if k > num:\n",
    "                break\n",
    "            model(image)\n",
    "\n",
    "shape = 1, 3, 32, 32\n",
    "model = resnet18()\n",
    "model.conv1 = nn.Conv2d(model.conv1.in_channels, \n",
    "                        model.conv1.out_channels,\n",
    "                        3, 1, 1, bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model_path = \"draft/params/resnet18_cifar10_sigmoid.h5\"\n",
    "model.load_state_dict(torch.load(model_path), strict=True)\n",
    "# script_module = torch.jit.trace(model.eval(), torch.rand(*shape))\n",
    "dataset = Cifar10(root=\"draft/data\", batch_size=1)\n",
    "trainset = dataset.train_loader() # 训练集\n",
    "qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\n",
    "example_input, _ = next(iter(trainset))\n",
    "model = prepare_fx(model.eval(), qconfig_mapping, (example_input,))\n",
    "calibrate(model, trainset) # 在样本数据上运行校准\n",
    "script_model = convert_fx(model)\n",
    "input_infos = [(\"data\", shape),]\n",
    "mod, params = from_pytorch(\n",
    "    script_module, input_infos,\n",
    "    custom_convert_map=None,\n",
    "    default_dtype='float32',\n",
    "    use_parser_friendly_name=False,\n",
    "    keep_quantized_weight=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:05, 34.16it/s]\n",
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torch/ao/quantization/utils.py:310: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Shapes of input list and information in the graph do not match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m script_model \u001b[39m=\u001b[39m convert_fx(model)\n\u001b[1;32m     30\u001b[0m input_infos \u001b[39m=\u001b[39m [(\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m, shape),]\n\u001b[0;32m---> 31\u001b[0m mod, params \u001b[39m=\u001b[39m from_pytorch(\n\u001b[1;32m     32\u001b[0m     script_module, input_infos,\n\u001b[1;32m     33\u001b[0m     custom_convert_map\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     34\u001b[0m     default_dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m     use_parser_friendly_name\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     36\u001b[0m     keep_quantized_weight\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m )\n",
      "File \u001b[0;32m/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/__pypackages__/3.10/lib/tvm/relay/frontend/pytorch.py:4939\u001b[0m, in \u001b[0;36mfrom_pytorch\u001b[0;34m(script_module, input_infos, custom_convert_map, default_dtype, use_parser_friendly_name, keep_quantized_weight, export_renamed_c_graph_path)\u001b[0m\n\u001b[1;32m   4937\u001b[0m is_module \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(script_module, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule)\n\u001b[1;32m   4938\u001b[0m params \u001b[39m=\u001b[39m script_module\u001b[39m.\u001b[39mstate_dict() \u001b[39mif\u001b[39;00m is_module \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 4939\u001b[0m outputs \u001b[39m=\u001b[39m _get_relay_input_vars(\n\u001b[1;32m   4940\u001b[0m     graph, input_infos, prelude, default_dtype\u001b[39m=\u001b[39;49mdefault_dtype, is_module\u001b[39m=\u001b[39;49mis_module\n\u001b[1;32m   4941\u001b[0m )\n\u001b[1;32m   4943\u001b[0m \u001b[39mif\u001b[39;00m use_parser_friendly_name:\n\u001b[1;32m   4944\u001b[0m     new_names \u001b[39m=\u001b[39m [key\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mkeys()]\n",
      "File \u001b[0;32m/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/__pypackages__/3.10/lib/tvm/relay/frontend/pytorch.py:4698\u001b[0m, in \u001b[0;36m_get_relay_input_vars\u001b[0;34m(graph, input_infos, prelude, is_module, default_dtype)\u001b[0m\n\u001b[1;32m   4695\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4696\u001b[0m         new_input_infos\u001b[39m.\u001b[39mappend(inp)\n\u001b[0;32m-> 4698\u001b[0m input_types \u001b[39m=\u001b[39m [\n\u001b[1;32m   4699\u001b[0m     (name, get_relay_ty(info[\u001b[39m0\u001b[39m], info[\u001b[39m1\u001b[39m], gi\u001b[39m.\u001b[39mtype()))\n\u001b[1;32m   4700\u001b[0m     \u001b[39mfor\u001b[39;00m (name, info), gi \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(new_input_infos, graph_inputs)\n\u001b[1;32m   4701\u001b[0m ]\n\u001b[1;32m   4703\u001b[0m ir_inputs \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39mdebugName() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m graph_inputs]\n\u001b[1;32m   4704\u001b[0m \u001b[39mfor\u001b[39;00m ir_input, (name, itype) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(ir_inputs, input_types):\n",
      "File \u001b[0;32m/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/__pypackages__/3.10/lib/tvm/relay/frontend/pytorch.py:4699\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   4695\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4696\u001b[0m         new_input_infos\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m   4698\u001b[0m input_types \u001b[39m=\u001b[39m [\n\u001b[0;32m-> 4699\u001b[0m     (name, get_relay_ty(info[\u001b[39m0\u001b[39;49m], info[\u001b[39m1\u001b[39;49m], gi\u001b[39m.\u001b[39;49mtype()))\n\u001b[1;32m   4700\u001b[0m     \u001b[39mfor\u001b[39;00m (name, info), gi \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(new_input_infos, graph_inputs)\n\u001b[1;32m   4701\u001b[0m ]\n\u001b[1;32m   4703\u001b[0m ir_inputs \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39mdebugName() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m graph_inputs]\n\u001b[1;32m   4704\u001b[0m \u001b[39mfor\u001b[39;00m ir_input, (name, itype) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(ir_inputs, input_types):\n",
      "File \u001b[0;32m/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/__pypackages__/3.10/lib/tvm/relay/frontend/pytorch.py:4644\u001b[0m, in \u001b[0;36m_get_relay_input_vars.<locals>.get_relay_ty\u001b[0;34m(ishape, itype, pt_type)\u001b[0m\n\u001b[1;32m   4639\u001b[0m \u001b[39mif\u001b[39;00m (pt_type\u001b[39m.\u001b[39mdim() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m pt_type\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(ishape)) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   4640\u001b[0m     pt_type\u001b[39m.\u001b[39msizes() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4641\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m([s1 \u001b[39m!=\u001b[39m s2 \u001b[39mfor\u001b[39;00m s1, s2 \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(pt_type\u001b[39m.\u001b[39msizes(), ishape)])\n\u001b[1;32m   4642\u001b[0m ):\n\u001b[1;32m   4643\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mShapes of input list and information in the graph do not match\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 4644\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m   4645\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(ishape) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(dim \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m ishape[\u001b[39m1\u001b[39m:]):\n\u001b[1;32m   4646\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m   4647\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected input\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms non-batch dimensions to have positive length, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4648\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut input has a shape of \u001b[39m\u001b[39m{\u001b[39;00mpt_type\u001b[39m.\u001b[39msizes()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4649\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Shapes of input list and information in the graph do not match"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import get_default_qconfig_mapping #QConfigMapping\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from draft.resnet_prelu import resnet18\n",
    "from draft.dataset import Cifar10\n",
    "\n",
    "def calibrate(model, data_loader, num=200):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k, (image, _) in tqdm(enumerate(data_loader)):\n",
    "            if k > num:\n",
    "                break\n",
    "            model(image)\n",
    "\n",
    "shape = (1, 3, 32, 32)\n",
    "model = resnet18()\n",
    "model.conv1 = nn.Conv2d(model.conv1.in_channels, \n",
    "                        model.conv1.out_channels,\n",
    "                        3, 1, 1, bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "model_path = \"draft/params/resnet18_cifar10_prelu.h5\"\n",
    "model.load_state_dict(torch.load(model_path), strict=True)\n",
    "# script_module = torch.jit.trace(model.eval(), torch.rand(*shape))\n",
    "dataset = Cifar10(root=\"draft/data\", batch_size=1)\n",
    "trainset = dataset.train_loader() # 训练集\n",
    "qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\n",
    "example_input, _ = next(iter(trainset))\n",
    "model = prepare_fx(model.eval(), qconfig_mapping, (example_input,))\n",
    "calibrate(model, trainset) # 在样本数据上运行校准\n",
    "script_model = convert_fx(model)\n",
    "input_infos = [(\"data\", shape),]\n",
    "mod, params = from_pytorch(\n",
    "    script_module, input_infos,\n",
    "    custom_convert_map=None,\n",
    "    default_dtype='float32',\n",
    "    use_parser_friendly_name=False,\n",
    "    keep_quantized_weight=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mod[\"main\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm.relay.frontend import qnn_torch\n",
    "from tvm.relay.frontend.pytorch import (\n",
    "    _run_jit_passes,\n",
    "    Prelude, PyTorchOpConverter,\n",
    "    get_all_op_names,\n",
    "    _get_relay_input_vars,\n",
    "    _debug_rename,\n",
    "    convert_params,\n",
    "    _get_output_name,\n",
    "    get_attr_chains,\n",
    "    _getattr_full_name,\n",
    "    _get_users,\n",
    "    getattr_attr_name,\n",
    "    _get_tensor_and_var\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_parser_friendly_name = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = tvm.IRModule()\n",
    "prelude = Prelude(mod)\n",
    "enable_lower_all_tuples = True\n",
    "\n",
    "converter = PyTorchOpConverter(prelude, default_dtype, use_parser_friendly_name)\n",
    "graph = script_module.graph.copy()\n",
    "graph_inputs = list(graph.inputs())\n",
    "_run_jit_passes(graph, enable_lower_all_tuples)\n",
    "op_names = get_all_op_names(graph)\n",
    "converter.report_missing_conversion(op_names)\n",
    "is_module = isinstance(script_module, torch.jit.ScriptModule)\n",
    "params = script_module.state_dict() if is_module else {}\n",
    "outputs = _get_relay_input_vars(\n",
    "    graph, input_infos, prelude, default_dtype=default_dtype, is_module=is_module\n",
    ")\n",
    "source_map = _debug_rename(graph, use_parser_friendly_name)\n",
    "param_vars, tensors, packed_param_map, param_debug_name_map = convert_params(\n",
    "    graph, params, source_map, use_parser_friendly_name\n",
    ")\n",
    "tvm_params = {k: tvm.nd.array(v) for k, v in tensors.items()}\n",
    "outputs.update(param_vars)\n",
    "quantized_ops = set([\"aten::quantize_per_tensor\", \"quantized::linear_dynamic\"])\n",
    "if len(quantized_ops.intersection(set(op_names))) > 0:\n",
    "    weight_quant_params = qnn_torch.get_weight_quant_params(\n",
    "        script_module, packed_param_map.values()\n",
    "    )\n",
    "    qnn_torch.inline_input_quant_params_for_fx(graph, tensors, param_debug_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = params\n",
    "# getattr_nodes = graph.findAllNodes(\"prim::GetAttr\", recurse=True)\n",
    "# params = {}\n",
    "# param_tensors = {}\n",
    "# packed_param_map = {}\n",
    "# param_debug_name_map = {}\n",
    "# vars_by_name = {}\n",
    "# seen = set()\n",
    "# attr_name_sep = \"_\" if use_parser_friendly_name else \".\"\n",
    "\n",
    "# for node in getattr_nodes:\n",
    "#     if _get_output_name(node) in seen:\n",
    "#         continue\n",
    "\n",
    "#     for getattrs in get_attr_chains(node):\n",
    "#         seen.update(map(_get_output_name, getattrs))\n",
    "\n",
    "#         full_attr = _getattr_full_name(getattrs, attr_name_sep)\n",
    "#         full_attr_node_name = _get_output_name(getattrs[-1])\n",
    "#         print(full_attr, full_attr_node_name)\n",
    "#         # set variable name by concatenating first consumer's name with full attribute\n",
    "#         # e.g. \"aten::batch_norm_5.running_mean\"\n",
    "#         var_name = attr_name_sep.join(\n",
    "#             [source_map[_get_users(getattrs[-1])[0]], full_attr.split(attr_name_sep)[-1]]\n",
    "#         )\n",
    "\n",
    "#         if full_attr.endswith(\"_packed_params\"):  # for quantized models\n",
    "#             packed_param_map[full_attr_node_name] = full_attr\n",
    "#         elif full_attr in state_dict:\n",
    "#             if var_name in vars_by_name:\n",
    "#                 var = vars_by_name[var_name]\n",
    "#             else:\n",
    "#                 torch_tensor = state_dict[full_attr]\n",
    "#                 tensor, var = _get_tensor_and_var(torch_tensor, var_name)\n",
    "#                 param_tensors[var_name] = tensor\n",
    "#                 # for quantized parameters to be correctly located\n",
    "#                 param_debug_name_map[full_attr_node_name] = var_name\n",
    "#                 vars_by_name[var_name] = var\n",
    "#             params[full_attr_node_name] = var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_attr_name(current):\n",
    "    current_attr = getattr_attr_name(current)\n",
    "    inputs = list(current.inputs())\n",
    "    # logging.debug(f\"current_attr: {current_attr}\")\n",
    "    if len(inputs) == 1:\n",
    "        # logging.debug(f\"get_full_attr_name(inputs[0].node()): {inputs[0].node()}\")\n",
    "        if inputs[0].node().kind() == \"prim::GetAttr\":\n",
    "            return get_full_attr_name(inputs[0].node()) + \".\" + current_attr\n",
    "        elif inputs[0].node().kind() == \"prim::Param\":\n",
    "            return current_attr + \".1\"\n",
    "    return current_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in graph.findAllNodes(\"prim::GetAttr\", recurse=True):\n",
    "    out_name = node.output().debugName()\n",
    "    if \"_scale\" in out_name or \"_zero_point\" in out_name:\n",
    "        full_attr = param_debug_name_map[get_full_attr_name(node)]\n",
    "        assert full_attr in params, f\"{full_attr} not found in param dict.\"\n",
    "        param_np = params[full_attr].asnumpy()\n",
    "        new_const_node = graph.create(\"prim::Constant\")\n",
    "        new_const_node.insertBefore(node)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_attr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = node\n",
    "getattr_attr_name(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = node\n",
    "current_attr = getattr_attr_name(current)\n",
    "inputs = list(current.inputs())\n",
    "input_node = inputs[0].node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_node.kind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for getattrs in get_attr_chains(input_node):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_full_attr_name(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.output().debugName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_attr = param_debug_name_map[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_map, op_type_dict = {}, {}\n",
    "prim_with_blocks = [\"prim::If\", \"prim::Loop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in graph.nodes():\n",
    "    if node.outputsSize() == 0:\n",
    "        continue\n",
    "    if node.kind() in prim_with_blocks:\n",
    "        for block in node.blocks():\n",
    "            _traverse_graph(block.nodes())\n",
    "    _rename_outputs(node, source_map, op_type_dict, use_parser_friendly_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.outputsSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
