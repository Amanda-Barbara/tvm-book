{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow2 Keras 推理\n",
    "\n",
    "\n",
    "```{topic} 主题\n",
    "TVM 对 TensorFlow1 模型的支持并不完善，为了提高开发效率，本节讨论如何将 TensorFlow1 模型转换为 TensorFlow2 的 Keras 模型，最终再将其转换为 ONNX 或者 TFLite 模型。\n",
    "```\n",
    "\n",
    "参考：[migrating_checkpoints](https://www.tensorflow.org/guide/migrate/migrating_checkpoints)\n",
    "\n",
    "下面以模型 [resnet_v2_50](http://download.tensorflow.org/models/resnet_v2_50_2017_04_14.tar.gz) 为例展示。\n",
    "\n",
    "需要克隆项目 [models](https://github.com/tensorflow/models)，然后执行如下操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 13:33:32.794252: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-17 13:33:35.626594: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-17 13:33:35.644448: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-17 13:33:55.703030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "try:\n",
    "    tf1 = tf.compat.v1\n",
    "except (ImportError, AttributeError):\n",
    "    tf1 = tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切换到 `models/research/slim` 目录下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pc/data/lxw/ai/tasks/models/research/slim\n"
     ]
    }
   ],
   "source": [
    "%cd /media/pc/data/lxw/ai/tasks/models/research/slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 TF1 升级为 TF2："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets import resnet_v2\n",
    "import tf_slim as slim\n",
    "\n",
    "\n",
    "class ResnetV2_50_block(tf.keras.layers.Layer):\n",
    "    def __init__(self, trainable=False, \n",
    "                 name=\"resnet_v2_50\", \n",
    "                 dtype=None, dynamic=False, **kwargs):\n",
    "        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n",
    "\n",
    "    @tf1.keras.utils.track_tf1_style_variables\n",
    "    def call(self, inputs):\n",
    "        with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n",
    "            logits, end_points = resnet_v2.resnet_v2_50(\n",
    "                inputs, \n",
    "                num_classes=1001,\n",
    "                global_pool=True,\n",
    "                is_training=self.trainable,\n",
    "                scope=self.name\n",
    "            )\n",
    "        del end_points\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "class ResnetV2_50(tf.keras.Model):\n",
    "    def __init__(self, trainable=False, \n",
    "                 name=\"resnet_v2_50\", \n",
    "                 dtype=None, dynamic=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.block = ResnetV2_50_block(\n",
    "            trainable=trainable, \n",
    "            name=name, dtype=dtype, \n",
    "            dynamic=dynamic, **kwargs\n",
    "        )\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None, 224, 224, 3], \n",
    "                                                 tf.float32, name=\"data\")])\n",
    "    def call(self, inputs):\n",
    "        x = self.block(inputs)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 13:34:44.643279: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from nets import resnet_v2\n",
    "from preprocessing.preprocessing_factory import get_preprocessing\n",
    "import tf_slim as slim\n",
    "\n",
    "preprocessing = get_preprocessing(\"resnet_v2_50\")\n",
    "image_size = 224\n",
    "path = '/media/pc/data/board/arria10/lxw/data/test/cat.png' # 将要预测的图片路径\n",
    "preprocessing = get_preprocessing(\"resnet_v2_50\")\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess_image(image, output_height, output_width):\n",
    "    # image = tf.constant(image)\n",
    "    processed_image = preprocessing(image, output_height, output_width)\n",
    "    return processed_image/256\n",
    "with Image.open(path) as im:\n",
    "    if im.mode != \"RGB\":\n",
    "        im.convert(\"RGB\")\n",
    "    image = np.asarray(im)\n",
    "np_processed_image = preprocess_image(image, image_size, image_size)\n",
    "np_processed_images = np.expand_dims(np_processed_image, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/tensorflow/python/keras/engine/base_layer.py:2212: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/tensorflow/python/keras/engine/base_layer.py:1345: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n",
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/keras/legacy_tf_layers/base.py:627: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  self.updates, tf.compat.v1.GraphKeys.UPDATE_OPS\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error when restoring from checkpoint or SavedModel at /tmp/checkpoints/resnet_v2_50.ckpt: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /tmp/checkpoints/resnet_v2_50.ckpt\nPlease double-check that the path is correct. You may be missing the checkpoint suffix (e.g. the '-1' in 'path/to/ckpt-1').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/tensorflow/python/training/py_checkpoint_reader.py:92\u001b[0m, in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m CheckpointReader(compat\u001b[39m.\u001b[39;49mas_bytes(filepattern))\n\u001b[1;32m     93\u001b[0m \u001b[39m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m# issue with throwing python exceptions from C++.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /tmp/checkpoints/resnet_v2_50.ckpt",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py:2591\u001b[0m, in \u001b[0;36mCheckpoint.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2590\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2591\u001b[0m   status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(save_path, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m   2592\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n",
      "File \u001b[0;32m/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py:2463\u001b[0m, in \u001b[0;36mCheckpoint.read\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2462\u001b[0m options \u001b[39m=\u001b[39m options \u001b[39mor\u001b[39;00m checkpoint_options\u001b[39m.\u001b[39mCheckpointOptions()\n\u001b[0;32m-> 2463\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_saver\u001b[39m.\u001b[39;49mrestore(save_path\u001b[39m=\u001b[39;49msave_path, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m   2464\u001b[0m metrics\u001b[39m.\u001b[39mAddCheckpointReadDuration(\n\u001b[1;32m   2465\u001b[0m     api_label\u001b[39m=\u001b[39m_CHECKPOINT_V2,\n\u001b[1;32m   2466\u001b[0m     microseconds\u001b[39m=\u001b[39m_get_duration_microseconds(start_time, time\u001b[39m.\u001b[39mtime()))\n",
      "File \u001b[0;32m/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py:1405\u001b[0m, in \u001b[0;36mTrackableSaver.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1403\u001b[0m   _ASYNC_CHECKPOINT_THREAD\u001b[39m.\u001b[39mjoin()\n\u001b[0;32m-> 1405\u001b[0m reader \u001b[39m=\u001b[39m py_checkpoint_reader\u001b[39m.\u001b[39;49mNewCheckpointReader(save_path)\n\u001b[1;32m   1406\u001b[0m graph_building \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/tensorflow/python/training/py_checkpoint_reader.py:96\u001b[0m, in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 96\u001b[0m   error_translator(e)\n",
      "File \u001b[0;32m/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/tensorflow/python/training/py_checkpoint_reader.py:31\u001b[0m, in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnot found in checkpoint\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m error_message \u001b[39mor\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFailed to find any \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmatching files for\u001b[39m\u001b[39m'\u001b[39m) \u001b[39min\u001b[39;00m error_message:\n\u001b[0;32m---> 31\u001b[0m   \u001b[39mraise\u001b[39;00m errors_impl\u001b[39m.\u001b[39mNotFoundError(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, error_message)\n\u001b[1;32m     32\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mSliced checkpoints are not supported\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m error_message \u001b[39mor\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mData type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msupported\u001b[39m\u001b[39m'\u001b[39m) \u001b[39min\u001b[39;00m error_message:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /tmp/checkpoints/resnet_v2_50.ckpt",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model(tf\u001b[39m.\u001b[39mones(shape\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m3\u001b[39m), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32))\n\u001b[1;32m      3\u001b[0m ckpt \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mCheckpoint(model\u001b[39m=\u001b[39mmodel)\n\u001b[0;32m----> 4\u001b[0m ckpt\u001b[39m.\u001b[39;49mrestore(\u001b[39m\"\u001b[39;49m\u001b[39m/tmp/checkpoints/resnet_v2_50.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m# 更新模型参数\u001b[39;00m\n\u001b[1;32m      5\u001b[0m outputs \u001b[39m=\u001b[39m model(np_processed_images)\n\u001b[1;32m      6\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py:2595\u001b[0m, in \u001b[0;36mCheckpoint.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2593\u001b[0m     context\u001b[39m.\u001b[39masync_wait()  \u001b[39m# Ensure restore operations have completed.\u001b[39;00m\n\u001b[1;32m   2594\u001b[0m \u001b[39mexcept\u001b[39;00m errors_impl\u001b[39m.\u001b[39mNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 2595\u001b[0m   \u001b[39mraise\u001b[39;00m errors_impl\u001b[39m.\u001b[39mNotFoundError(\n\u001b[1;32m   2596\u001b[0m       \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2597\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError when restoring from checkpoint or SavedModel at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2598\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00morig_save_path\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m.\u001b[39mmessage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2599\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mPlease double-check that the path is correct. You may be missing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2600\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mthe checkpoint suffix (e.g. the \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-1\u001b[39m\u001b[39m'\u001b[39m\u001b[39m in \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath/to/ckpt-1\u001b[39m\u001b[39m'\u001b[39m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2601\u001b[0m \u001b[39m# Create the save counter now so it gets initialized with other variables\u001b[39;00m\n\u001b[1;32m   2602\u001b[0m \u001b[39m# when graph building. Creating it earlier would lead to errors when using,\u001b[39;00m\n\u001b[1;32m   2603\u001b[0m \u001b[39m# say, train.Saver() to save the model before initializing it.\u001b[39;00m\n\u001b[1;32m   2604\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_create_save_counter()\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error when restoring from checkpoint or SavedModel at /tmp/checkpoints/resnet_v2_50.ckpt: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /tmp/checkpoints/resnet_v2_50.ckpt\nPlease double-check that the path is correct. You may be missing the checkpoint suffix (e.g. the '-1' in 'path/to/ckpt-1')."
     ]
    }
   ],
   "source": [
    "model = ResnetV2_50()\n",
    "model(tf.ones(shape=(1, 224, 224, 3), dtype=tf.float32))\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(\"/tmp/checkpoints/resnet_v2_50.ckpt\") # 更新模型参数\n",
    "outputs = model(np_processed_images)\n",
    "outputs = outputs.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印标签信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "\n",
    "g = Github(user_agent=\"xinetzone\")\n",
    "repo = g.get_repo(\"tensorflow/models\")\n",
    "label_content = repo.get_contents(\"research/slim/datasets/imagenet_lsvrc_2015_synsets.txt\")\n",
    "imagenet_labels = label_content.decoded_content.decode().split()\n",
    "assert len(imagenet_labels) == 1000\n",
    "metadata = repo.get_contents(\"research/slim/datasets/imagenet_metadata.txt\")\n",
    "imagenet_metadata = metadata.decoded_content.decode().splitlines()\n",
    "synset_to_human = {}\n",
    "for metadata in imagenet_metadata:\n",
    "    name, value = metadata.split(\"\\t\")\n",
    "    synset_to_human[name] = value\n",
    "name2id = {name: k+1 for k, name in enumerate(imagenet_labels)}\n",
    "\n",
    "topk = 5\n",
    "sorted_inds = outputs[0].argsort()[::-1]\n",
    "for sorted_ind in sorted_inds[:topk]:\n",
    "    label = synset_to_human[imagenet_labels[sorted_ind-1]]\n",
    "    print(f\"{sorted_ind-1}: {label.ljust(20)}\\t{outputs[0][sorted_ind-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将其模型和参数与加载下来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = ResnetV2_50()\n",
    "# inputs = tf.keras.Input(shape=(224, 224, 3), dtype=tf.float32, name=\"data\")\n",
    "# outputs = model(inputs)\n",
    "# model2 = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"resnet_v2_50_model\")\n",
    "\n",
    "# model2.save(module_with_signature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_with_signature_path = \"temp/resnet_v2_50_keras\"\n",
    "model.save(module_with_signature_path)\n",
    "imported_with_signatures = tf.saved_model.load(module_with_signature_path)\n",
    "infer = imported_with_signatures.signatures['serving_default']\n",
    "labeling = infer(tf.constant(np_processed_images))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转换为 ONNX 模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras 模型转换 ONNX](https://onnxruntime.ai/docs/tutorials/tf-get-started.html)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "input_signature = [tf.TensorSpec([1, 224, 224, 3], tf.float32, name=\"data\")]\n",
    "# Use from_function for tf functions\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n",
    "onnx.save(onnx_model, \"temp/resnet_v2_50.onnx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import set_env\n",
    "from tvm.relay.frontend import from_onnx\n",
    "\n",
    "shape_dict = {\"data\": [1, 224, 224, 3]}\n",
    "\n",
    "graph_def = onnx.load(\"temp/resnet_v2_50.onnx\")\n",
    "mod, params = from_onnx(\n",
    "    graph_def,\n",
    "    shape_dict,\n",
    "    opset=13,\n",
    "    freeze_params=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, \"llvm\", params=params)\n",
    "    \n",
    "inputs_dict = {\"data\": np_processed_images}\n",
    "mlib_proxy = tvm.contrib.graph_executor.GraphModule(lib[\"default\"](tvm.cpu()))\n",
    "mlib_proxy.run(**inputs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证一致性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(\n",
    "    labeling['output_1'].numpy(), \n",
    "    mlib_proxy.get_output(0).numpy(),\n",
    "    rtol=1e-07, atol=1e-5\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转换为 TFLite 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(module_with_signature_path)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('temp/resnet_v2_50.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载 TFLite 模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflite\n",
    "\n",
    "\n",
    "with open('temp/resnet_v2_50.tflite', \"rb\") as fp:\n",
    "    tflite_model_buf = fp.read()\n",
    "\n",
    "tflite_model = tflite.Model.GetRootAsModel(tflite_model_buf, 0)\n",
    "mod, params = relay.frontend.from_tflite(\n",
    "    tflite_model, shape_dict=shape_dict, \n",
    "    dtype_dict={\"data\": \"float32\"}\n",
    ")\n",
    "desired_layouts = {\n",
    "    # 'image.resize2d': ['NCHW'],\n",
    "    'nn.conv2d': ['NCHW', 'default'],\n",
    "    'nn.max_pool2d': ['NCHW', 'default'],\n",
    "    'nn.avg_pool2d': ['NCHW', 'default'],\n",
    "}\n",
    "# NHWC 将布局转换为 NCHW 且移除未使用算子\n",
    "seq = tvm.transform.Sequential([\n",
    "    relay.transform.RemoveUnusedFunctions(),\n",
    "    relay.transform.ConvertLayout(desired_layouts)\n",
    "])\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    mod = seq(mod)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证结果一致性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, \"llvm\", params=params)\n",
    "    \n",
    "inputs_dict = {\"data\": np_processed_images}\n",
    "mlib_proxy = tvm.contrib.graph_executor.GraphModule(lib[\"default\"](tvm.cpu()))\n",
    "mlib_proxy.run(**inputs_dict)\n",
    "np.testing.assert_allclose(\n",
    "    labeling['output_1'].numpy(), \n",
    "    mlib_proxy.get_output(0).numpy(),\n",
    "    rtol=1e-07, atol=1e-5\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "TFLite 转换出现了问题，暂时搁置。\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
