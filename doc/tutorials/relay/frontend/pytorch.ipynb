{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 前端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.ao.quantization import get_default_qat_qconfig_mapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class Demo(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(16, 64, 3, 1, 1, bias=False, groups=16)\n",
    "        # self.prelu = nn.PReLU(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv(x)\n",
    "        # x = self.prelu(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torch/ao/quantization/utils.py:310: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Demo()\n",
    "shape = 1, 16, 32, 32\n",
    "example_inputs = [torch.rand(*shape),]\n",
    "# script_module = torch.jit.trace(model.eval(), example_inputs)\n",
    "model_qat = torch.fx.symbolic_trace(model)\n",
    "model_qat = torch.fx.GraphModule(model_qat, model_qat.graph)\n",
    "qconfig_mapping = get_default_qat_qconfig_mapping(\"qnnpack\")\n",
    "model_prepared = prepare_qat_fx(model_qat, qconfig_mapping, example_inputs).eval()\n",
    "model_converted = convert_fx(model_prepared).eval()\n",
    "script_module = torch.jit.trace(model_converted.eval(), example_inputs).eval()\n",
    "input_infos = [(\"data\", shape),]\n",
    "default_dtype = \"float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.relay.frontend.pytorch import from_pytorch\n",
    "\n",
    "input_infos = [(\"data\", shape),]\n",
    "mod, params = from_pytorch(\n",
    "    script_module, input_infos,\n",
    "    custom_convert_map=None,\n",
    "    default_dtype='float32',\n",
    "    use_parser_friendly_name=False,\n",
    "    keep_quantized_weight=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = script_module.graph.copy()\n",
    "graph_inputs = list(graph.inputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[self.1 defined in (%self.1 : __torch__.torch.fx.graph_module.GraphModule, %x : Float(1, 16, 32, 32, strides=[16384, 1024, 32, 1], requires_grad=0, device=cpu) = prim::Param()\n",
       " ),\n",
       " x defined in (%self.1 : __torch__.torch.fx.graph_module.GraphModule, %x : Float(1, 16, 32, 32, strides=[16384, 1024, 32, 1], requires_grad=0, device=cpu) = prim::Param()\n",
       " )]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/doc/tutorials/relay/frontend/draft/resnet18_cifar10_relu_qat\"\n",
    "script_module = torch.jit.load(f\"{ROOT}/weight/resnet18_cifar10_relu_qat.h5\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'conv1_input_zero_point_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_infos \u001b[39m=\u001b[39m [(\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m, (\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)),]\n\u001b[0;32m----> 2\u001b[0m mod, params \u001b[39m=\u001b[39m from_pytorch(\n\u001b[1;32m      3\u001b[0m     script_module, input_infos,\n\u001b[1;32m      4\u001b[0m     custom_convert_map\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      5\u001b[0m     default_dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     use_parser_friendly_name\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      7\u001b[0m     keep_quantized_weight\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/__pypackages__/3.10/lib/tvm/relay/frontend/pytorch.py:4964\u001b[0m, in \u001b[0;36mfrom_pytorch\u001b[0;34m(script_module, input_infos, custom_convert_map, default_dtype, use_parser_friendly_name, keep_quantized_weight, export_renamed_c_graph_path)\u001b[0m\n\u001b[1;32m   4960\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(quantized_ops\u001b[39m.\u001b[39mintersection(\u001b[39mset\u001b[39m(op_names))) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   4961\u001b[0m     weight_quant_params \u001b[39m=\u001b[39m qnn_torch\u001b[39m.\u001b[39mget_weight_quant_params(\n\u001b[1;32m   4962\u001b[0m         script_module, packed_param_map\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m   4963\u001b[0m     )\n\u001b[0;32m-> 4964\u001b[0m     qnn_torch\u001b[39m.\u001b[39;49minline_input_quant_params_for_fx(graph, tensors, param_debug_name_map)\n\u001b[1;32m   4965\u001b[0m     input_scales_for_bias \u001b[39m=\u001b[39m qnn_torch\u001b[39m.\u001b[39madd_input_quant_params_to_op_inputs(graph)\n\u001b[1;32m   4966\u001b[0m     qnn_torch\u001b[39m.\u001b[39madd_quant_params_to_outputs(\n\u001b[1;32m   4967\u001b[0m         outputs,\n\u001b[1;32m   4968\u001b[0m         packed_param_map,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4971\u001b[0m         keep_quantized_weight,\n\u001b[1;32m   4972\u001b[0m     )\n",
      "File \u001b[0;32m/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/__pypackages__/3.10/lib/tvm/relay/frontend/qnn_torch.py:554\u001b[0m, in \u001b[0;36minline_input_quant_params_for_fx\u001b[0;34m(graph, params, param_debug_name_map)\u001b[0m\n\u001b[1;32m    551\u001b[0m out_name \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39moutput()\u001b[39m.\u001b[39mdebugName()\n\u001b[1;32m    553\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_scale\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m out_name \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_zero_point\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m out_name:\n\u001b[0;32m--> 554\u001b[0m     full_attr \u001b[39m=\u001b[39m param_debug_name_map[get_full_attr_name(node)]\n\u001b[1;32m    555\u001b[0m     \u001b[39massert\u001b[39;00m full_attr \u001b[39min\u001b[39;00m params, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfull_attr\u001b[39m}\u001b[39;00m\u001b[39m not found in param dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m     param_np \u001b[39m=\u001b[39m params[full_attr]\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'conv1_input_zero_point_0'"
     ]
    }
   ],
   "source": [
    "input_infos = [(\"data\", (1, 3, 224, 224)),]\n",
    "mod, params = from_pytorch(\n",
    "    script_module, input_infos,\n",
    "    custom_convert_map=None,\n",
    "    default_dtype='float32',\n",
    "    use_parser_friendly_name=False,\n",
    "    keep_quantized_weight=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm.relay.frontend import qnn_torch\n",
    "from tvm.relay.frontend.pytorch import (\n",
    "    _run_jit_passes,\n",
    "    Prelude, PyTorchOpConverter,\n",
    "    get_all_op_names,\n",
    "    _get_relay_input_vars,\n",
    "    _debug_rename,\n",
    "    convert_params,\n",
    "    _get_output_name,\n",
    "    get_attr_chains,\n",
    "    _getattr_full_name,\n",
    "    _get_users,\n",
    "    getattr_attr_name,\n",
    "    _get_tensor_and_var\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_parser_friendly_name = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'conv1_input_zero_point_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(quantized_ops\u001b[39m.\u001b[39mintersection(\u001b[39mset\u001b[39m(op_names))) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m     weight_quant_params \u001b[39m=\u001b[39m qnn_torch\u001b[39m.\u001b[39mget_weight_quant_params(\n\u001b[1;32m     25\u001b[0m         script_module, packed_param_map\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m     26\u001b[0m     )\n\u001b[0;32m---> 27\u001b[0m     qnn_torch\u001b[39m.\u001b[39;49minline_input_quant_params_for_fx(graph, tensors, param_debug_name_map)\n",
      "File \u001b[0;32m/media/pc/data/lxw/ai/tvm/xinetzone/tvm-book/__pypackages__/3.10/lib/tvm/relay/frontend/qnn_torch.py:554\u001b[0m, in \u001b[0;36minline_input_quant_params_for_fx\u001b[0;34m(graph, params, param_debug_name_map)\u001b[0m\n\u001b[1;32m    551\u001b[0m out_name \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39moutput()\u001b[39m.\u001b[39mdebugName()\n\u001b[1;32m    553\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_scale\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m out_name \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_zero_point\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m out_name:\n\u001b[0;32m--> 554\u001b[0m     full_attr \u001b[39m=\u001b[39m param_debug_name_map[get_full_attr_name(node)]\n\u001b[1;32m    555\u001b[0m     \u001b[39massert\u001b[39;00m full_attr \u001b[39min\u001b[39;00m params, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfull_attr\u001b[39m}\u001b[39;00m\u001b[39m not found in param dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m     param_np \u001b[39m=\u001b[39m params[full_attr]\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'conv1_input_zero_point_0'"
     ]
    }
   ],
   "source": [
    "mod = tvm.IRModule()\n",
    "prelude = Prelude(mod)\n",
    "enable_lower_all_tuples = True\n",
    "\n",
    "converter = PyTorchOpConverter(prelude, default_dtype, use_parser_friendly_name)\n",
    "graph = script_module.graph.copy()\n",
    "graph_inputs = list(graph.inputs())\n",
    "_run_jit_passes(graph, enable_lower_all_tuples)\n",
    "op_names = get_all_op_names(graph)\n",
    "converter.report_missing_conversion(op_names)\n",
    "is_module = isinstance(script_module, torch.jit.ScriptModule)\n",
    "params = script_module.state_dict() if is_module else {}\n",
    "outputs = _get_relay_input_vars(\n",
    "    graph, input_infos, prelude, default_dtype=default_dtype, is_module=is_module\n",
    ")\n",
    "source_map = _debug_rename(graph, use_parser_friendly_name)\n",
    "param_vars, tensors, packed_param_map, param_debug_name_map = convert_params(\n",
    "    graph, params, source_map, use_parser_friendly_name\n",
    ")\n",
    "tvm_params = {k: tvm.nd.array(v) for k, v in tensors.items()}\n",
    "outputs.update(param_vars)\n",
    "quantized_ops = set([\"aten::quantize_per_tensor\", \"quantized::linear_dynamic\"])\n",
    "if len(quantized_ops.intersection(set(op_names))) > 0:\n",
    "    weight_quant_params = qnn_torch.get_weight_quant_params(\n",
    "        script_module, packed_param_map.values()\n",
    "    )\n",
    "    qnn_torch.inline_input_quant_params_for_fx(graph, tensors, param_debug_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = params\n",
    "# getattr_nodes = graph.findAllNodes(\"prim::GetAttr\", recurse=True)\n",
    "# params = {}\n",
    "# param_tensors = {}\n",
    "# packed_param_map = {}\n",
    "# param_debug_name_map = {}\n",
    "# vars_by_name = {}\n",
    "# seen = set()\n",
    "# attr_name_sep = \"_\" if use_parser_friendly_name else \".\"\n",
    "\n",
    "# for node in getattr_nodes:\n",
    "#     if _get_output_name(node) in seen:\n",
    "#         continue\n",
    "\n",
    "#     for getattrs in get_attr_chains(node):\n",
    "#         seen.update(map(_get_output_name, getattrs))\n",
    "\n",
    "#         full_attr = _getattr_full_name(getattrs, attr_name_sep)\n",
    "#         full_attr_node_name = _get_output_name(getattrs[-1])\n",
    "#         print(full_attr, full_attr_node_name)\n",
    "#         # set variable name by concatenating first consumer's name with full attribute\n",
    "#         # e.g. \"aten::batch_norm_5.running_mean\"\n",
    "#         var_name = attr_name_sep.join(\n",
    "#             [source_map[_get_users(getattrs[-1])[0]], full_attr.split(attr_name_sep)[-1]]\n",
    "#         )\n",
    "\n",
    "#         if full_attr.endswith(\"_packed_params\"):  # for quantized models\n",
    "#             packed_param_map[full_attr_node_name] = full_attr\n",
    "#         elif full_attr in state_dict:\n",
    "#             if var_name in vars_by_name:\n",
    "#                 var = vars_by_name[var_name]\n",
    "#             else:\n",
    "#                 torch_tensor = state_dict[full_attr]\n",
    "#                 tensor, var = _get_tensor_and_var(torch_tensor, var_name)\n",
    "#                 param_tensors[var_name] = tensor\n",
    "#                 # for quantized parameters to be correctly located\n",
    "#                 param_debug_name_map[full_attr_node_name] = var_name\n",
    "#                 vars_by_name[var_name] = var\n",
    "#             params[full_attr_node_name] = var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_attr_name(current):\n",
    "    current_attr = getattr_attr_name(current)\n",
    "    inputs = list(current.inputs())\n",
    "    # logging.debug(f\"current_attr: {current_attr}\")\n",
    "    if len(inputs) == 1:\n",
    "        # logging.debug(f\"get_full_attr_name(inputs[0].node()): {inputs[0].node()}\")\n",
    "        if inputs[0].node().kind() == \"prim::GetAttr\":\n",
    "            return get_full_attr_name(inputs[0].node()) + \".\" + current_attr\n",
    "        elif inputs[0].node().kind() == \"prim::Param\":\n",
    "            return current_attr + \".1\"\n",
    "    return current_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in graph.findAllNodes(\"prim::GetAttr\", recurse=True):\n",
    "    out_name = node.output().debugName()\n",
    "    if \"_scale\" in out_name or \"_zero_point\" in out_name:\n",
    "        full_attr = param_debug_name_map[get_full_attr_name(node)]\n",
    "        assert full_attr in params, f\"{full_attr} not found in param dict.\"\n",
    "        param_np = params[full_attr].asnumpy()\n",
    "        new_const_node = graph.create(\"prim::Constant\")\n",
    "        new_const_node.insertBefore(node)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aten::quantize_per_tensor_0.conv1_input_zero_point_0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_attr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = node\n",
    "getattr_attr_name(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = node\n",
    "current_attr = getattr_attr_name(current)\n",
    "inputs = list(current.inputs())\n",
    "input_node = inputs[0].node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_node.kind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for getattrs in get_attr_chains(input_node):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_full_attr_name(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.output().debugName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_attr = param_debug_name_map[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_map, op_type_dict = {}, {}\n",
    "prim_with_blocks = [\"prim::If\", \"prim::Loop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in graph.nodes():\n",
    "    if node.outputsSize() == 0:\n",
    "        continue\n",
    "    if node.kind() in prim_with_blocks:\n",
    "        for block in node.blocks():\n",
    "            _traverse_graph(block.nodes())\n",
    "    _rename_outputs(node, source_map, op_type_dict, use_parser_friendly_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.outputsSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
