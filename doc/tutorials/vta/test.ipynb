{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VTA 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D 卷积\n",
    "使用 NCHW 布局的卷积的数学定义：\n",
    "\n",
    "$$\n",
    "\\text{Conv}[b, k, i, j] =\n",
    "\\sum_{d_i, d_j, q} A[b, q, \\text{strides} * i + d_i, \\text{strides} * j + d_j] * W[k, q, d_i, d_j],\n",
    "$$\n",
    "\n",
    "其中 $A$ 是输入张量，$W$ 是权重张量， $W$ 是权重张量， $b$ 是批次索引， $k$ 是输出通道索引， $i$ 和 $j$ 是图像高度和宽度的索引，$d_i$ 和 $d_j$ 是权重的索引，$q$ 是输入通道，$\\text{strides}$ 是卷积核的步幅。\n",
    "\n",
    "下面考虑简单的情况：`stride=1, padding=0`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import set_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import tir as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
    "OUT_H, OUT_W = H - K + 1, W - K + 1\n",
    "data = np.arange(N*CI*H*W).reshape(N, CI, H, W).astype(np.int32)\n",
    "weight = np.arange(CO*CI*K*K).reshape(CO, CI, K, K).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 474,  510,  546,  582,  618,  654],\n",
       "         [ 762,  798,  834,  870,  906,  942],\n",
       "         [1050, 1086, 1122, 1158, 1194, 1230],\n",
       "         [1338, 1374, 1410, 1446, 1482, 1518],\n",
       "         [1626, 1662, 1698, 1734, 1770, 1806],\n",
       "         [1914, 1950, 1986, 2022, 2058, 2094]],\n",
       "\n",
       "        [[1203, 1320, 1437, 1554, 1671, 1788],\n",
       "         [2139, 2256, 2373, 2490, 2607, 2724],\n",
       "         [3075, 3192, 3309, 3426, 3543, 3660],\n",
       "         [4011, 4128, 4245, 4362, 4479, 4596],\n",
       "         [4947, 5064, 5181, 5298, 5415, 5532],\n",
       "         [5883, 6000, 6117, 6234, 6351, 6468]]]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch version\n",
    "import torch\n",
    "\n",
    "data_torch = torch.from_numpy(data)\n",
    "weight_torch = torch.from_numpy(weight)\n",
    "conv_torch = torch.nn.functional.conv2d(data_torch, weight_torch)\n",
    "conv_torch = conv_torch.numpy().astype(np.int32)\n",
    "conv_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyConv:\n",
    "  @T.prim_func\n",
    "  def conv(data: T.Buffer[(N, CI, HI, WI), \"int32\"],\n",
    "           weight: T.Buffer[(CO, CI, K, K), \"int32\"],\n",
    "           Y: T.Buffer[(N, CO, HO, WO), \"int32\"]):\n",
    "    T.func_attr({\"global_symbol\": \"conv\", \"tir.noalias\": True})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rt_lib = tvm.build(MyConv, target=\"llvm\")\n",
    "data_tvm = tvm.nd.array(data)\n",
    "weight_tvm = tvm.nd.array(weight)\n",
    "conv_tvm = tvm.nd.array(np.empty((N, CO, OUT_H, OUT_W), dtype=np.int64))\n",
    "rt_lib[\"conv\"](data_tvm, weight_tvm, conv_tvm)\n",
    "np.testing.assert_allclose(conv_tvm.numpy(), conv_torch, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class Add:\n",
    "    @T.prim_func\n",
    "    def add(A: T.Buffer[(4, 4), \"int64\"],\n",
    "            B: T.Buffer[(4, 4), \"int64\"],\n",
    "            C: T.Buffer[(4, 4), \"int64\"]):\n",
    "        T.func_attr({\"global_symbol\": \"add\"})\n",
    "        for i, j in T.grid(4, 4):\n",
    "            with T.block(\"C\"):\n",
    "                vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "                C[vi, vj] = A[vi, vj] + B[vi, vj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low-level numpy version\n",
    "def lnumpy_add(a: np.ndarray, b: np.ndarray, c: np.ndarray):\n",
    "  for i in range(4):\n",
    "    for j in range(4):\n",
    "      c[i, j] = a[i, j] + b[i, j]\n",
    "c_lnumpy = np.empty((4, 4), dtype=np.int64)\n",
    "lnumpy_add(a, b, c_lnumpy)\n",
    "c_lnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_lib = tvm.build(Add, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4, 4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_lib = tvm.build(Add, target=\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// tvm target: c -keys=cpu \n",
      "#define TVM_EXPORTS\n",
      "#include \"tvm/runtime/c_runtime_api.h\"\n",
      "#include \"tvm/runtime/c_backend_api.h\"\n",
      "#include <math.h>\n",
      "#ifdef __cplusplus\n",
      "extern \"C\"\n",
      "#endif\n",
      "TVM_DLL int32_t add(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n",
      "  void* arg_A_handle = (((TVMValue*)args)[0].v_handle);\n",
      "  int32_t arg_A_handle_code = arg_type_ids[0];\n",
      "  void* arg_B_handle = (((TVMValue*)args)[1].v_handle);\n",
      "  int32_t arg_B_handle_code = arg_type_ids[1];\n",
      "  void* arg_C_handle = (((TVMValue*)args)[2].v_handle);\n",
      "  int32_t arg_C_handle_code = arg_type_ids[2];\n",
      "  void* A = (((DLTensor*)arg_A_handle)[0].data);\n",
      "  void* arg_A_handle_shape = (((DLTensor*)arg_A_handle)[0].shape);\n",
      "  void* arg_A_handle_strides = (((DLTensor*)arg_A_handle)[0].strides);\n",
      "  int32_t dev_id = (((DLTensor*)arg_A_handle)[0].device.device_id);\n",
      "  void* B = (((DLTensor*)arg_B_handle)[0].data);\n",
      "  void* arg_B_handle_shape = (((DLTensor*)arg_B_handle)[0].shape);\n",
      "  void* arg_B_handle_strides = (((DLTensor*)arg_B_handle)[0].strides);\n",
      "  void* C = (((DLTensor*)arg_C_handle)[0].data);\n",
      "  void* arg_C_handle_shape = (((DLTensor*)arg_C_handle)[0].shape);\n",
      "  void* arg_C_handle_strides = (((DLTensor*)arg_C_handle)[0].strides);\n",
      "  if (!(arg_A_handle_strides == NULL)) {\n",
      "  }\n",
      "  if (!(arg_B_handle_strides == NULL)) {\n",
      "  }\n",
      "  if (!(arg_C_handle_strides == NULL)) {\n",
      "  }\n",
      "  for (int32_t i = 0; i < 4; ++i) {\n",
      "    for (int32_t j = 0; j < 4; ++j) {\n",
      "      int32_t cse_var_1 = ((i * 4) + j);\n",
      "      ((int64_t*)C)[cse_var_1] = (((int64_t*)A)[cse_var_1] + ((int64_t*)B)[cse_var_1]);\n",
      "    }\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "// CodegenC: NOTE: Auto-generated entry function\n",
      "#ifdef __cplusplus\n",
      "extern \"C\"\n",
      "#endif\n",
      "TVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n",
      "  return add(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rt_lib.get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
    "OUT_H, OUT_W = H - K + 1, W - K + 1\n",
    "data = np.arange(N*CI*H*W).reshape(N, CI, H, W)\n",
    "weight = np.arange(CO*CI*K*K).reshape(CO, CI, K, K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0d307675f12182d62ca143bf4e5db321e57c24ab1edf40ce60a9751b29adda0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
